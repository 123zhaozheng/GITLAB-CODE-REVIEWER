"""
ç®€åŒ–AIå¤„ç†å™¨æ¨¡å—
ç›´æ¥ä½¿ç”¨OpenAI APIï¼Œé¿å…LiteLLMçš„å¤æ‚æ€§
"""
import asyncio
import json
import re
import time
from typing import Dict, List, Optional, Any
import tiktoken
import openai
import logging

from config.settings import settings, MODEL_COSTS, REVIEW_TYPES
from core.gitlab_client import FilePatchInfo

logger = logging.getLogger(__name__)

# JSON Schemaå®šä¹‰ - ç”¨äºç»“æ„åŒ–è¾“å‡º
CODE_REVIEW_SCHEMA = {
    "type": "object",
    "properties": {
        "findings": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "type": {"type": "string", "description": "é—®é¢˜ç±»å‹"},
                    "filename": {"type": "string", "description": "æ–‡ä»¶å"},
                    "line_number": {"type": "integer", "description": "è¡Œå·"},
                    "severity": {
                        "type": "string", 
                        "enum": ["high", "medium", "low"],
                        "description": "ä¸¥é‡ç¨‹åº¦"
                    },
                    "description": {"type": "string", "description": "é—®é¢˜æè¿°"},
                    "suggestion": {"type": "string", "description": "ä¿®å¤å»ºè®®"}
                },
                "required": ["type", "filename", "severity", "description"]
            }
        },
        "suggestions": {
            "type": "array",
            "items": {"type": "string"},
            "description": "æ”¹è¿›å»ºè®®åˆ—è¡¨"
        },
        "overall_assessment": {
            "type": "string",
            "description": "æ•´ä½“è¯„ä¼°"
        }
    },
    "required": ["findings", "suggestions", "overall_assessment"]
}

SECURITY_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "is_vulnerability": {"type": "boolean", "description": "æ˜¯å¦ä¸ºå®‰å…¨æ¼æ´"},
        "risk_level": {
            "type": "integer", 
            "minimum": 1, 
            "maximum": 10,
            "description": "é£é™©çº§åˆ«(1-10)"
        },
        "description": {"type": "string", "description": "è¯¦ç»†æè¿°"},
        "fix_suggestion": {"type": "string", "description": "ä¿®å¤å»ºè®®"}
    },
    "required": ["is_vulnerability", "risk_level", "description", "fix_suggestion"]
}

PERFORMANCE_ANALYSIS_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "type": {"type": "string", "description": "æ€§èƒ½é—®é¢˜ç±»å‹"},
            "severity": {
                "type": "string",
                "enum": ["high", "medium", "low"],
                "description": "ä¸¥é‡ç¨‹åº¦"
            },
            "description": {"type": "string", "description": "é—®é¢˜æè¿°"},
            "optimization": {"type": "string", "description": "ä¼˜åŒ–å»ºè®®"}
        },
        "required": ["type", "severity", "description", "optimization"]
    }
}

class TokenManager:
    """Tokenç®¡ç†å™¨"""
    
    def __init__(self, model: str = None):
        self.model = model or settings.default_ai_model
        try:
            if "gpt" in self.model:
                self.encoder = tiktoken.encoding_for_model(self.model)
            else:
                self.encoder = tiktoken.get_encoding("o200k_base")
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ cl100k_base ç¼–ç 
            try:
                self.encoder = tiktoken.get_encoding("cl100k_base")
            except:
                # æœ€åå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ GPT-3.5 çš„ç¼–ç 
                self.encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬tokenæ•°é‡"""
        try:
            return len(self.encoder.encode(text))
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæŒ‰å­—ç¬¦æ•°ä¼°ç®—
            return len(text) // 4
    
    def estimate_cost(self, input_tokens: int, output_tokens: int = 1000) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        costs = MODEL_COSTS.get(self.model, {"input": 0.01, "output": 0.03})
        return (input_tokens * costs["input"] + output_tokens * costs["output"]) / 1000

class SimpleOpenAIClient:
    """ç®€åŒ–çš„OpenAIå®¢æˆ·ç«¯ - æœ€å°åŒ–åˆå§‹åŒ–å‚æ•°é¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜"""
    
    def __init__(self, api_key: str, base_url: Optional[str] = None):
        if not api_key:
            raise ValueError("API key is required")
            
        try:
            # ä½¿ç”¨æœ€åŸºæœ¬çš„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
            # æ˜ç¡®åªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼Œé¿å…ä¼ é€’ä¸å…¼å®¹çš„å‚æ•°å¦‚proxies
            client_kwargs = {
                "api_key": api_key,
                "timeout": 60.0,  # æ˜ç¡®è®¾ç½®è¶…æ—¶
            }
            
            if base_url:
                client_kwargs["base_url"] = base_url
                logger.info(f"OpenAI client initializing with custom base URL: {base_url}")
            
            self.client = openai.AsyncOpenAI(**client_kwargs)
            logger.info("OpenAI client initialized successfully")
                
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            # å¦‚æœbase_urlå¯¼è‡´é—®é¢˜ï¼Œå°è¯•ä»…ä½¿ç”¨API key
            if base_url:
                try:
                    logger.warning("Retrying without custom base URL")
                    self.client = openai.AsyncOpenAI(
                        api_key=api_key,
                        timeout=60.0
                    )
                    logger.info("OpenAI client initialized with default settings as fallback")
                except Exception as e2:
                    raise RuntimeError(f"Cannot initialize OpenAI client: {e2}")
            else:
                raise RuntimeError(f"Cannot initialize OpenAI client: {e}")
    
    async def chat_completion(self, messages: List[Dict], model: str, 
                            response_format: Optional[Dict] = None, **kwargs) -> str:
        """å‘é€èŠå¤©å®Œæˆè¯·æ±‚ - å¸¦è¯¦ç»†æ—¥å¿—è®°å½•"""
        import time
        
        # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è®¡ç®—è¾“å…¥tokenæ•°é‡
        input_text = "\n".join([msg.get('content', '') for msg in messages])
        input_tokens = len(input_text) // 4  # ç®€å•ä¼°ç®—
        
        # è®°å½•è¯·æ±‚å‚æ•°è¯¦æƒ…
        logger.info("=" * 60)
        logger.info("ğŸš€ OpenAI API è°ƒç”¨å¼€å§‹")
        logger.info("=" * 60)
        logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°:")
        logger.info(f"  - æ¨¡å‹: {model}")
        logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
        logger.info(f"  - ä¼°ç®—è¾“å…¥tokens: {input_tokens}")
        logger.info(f"  - é¢å¤–å‚æ•°: {kwargs}")
        
        # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆå¯é€‰æ‹©æ€§è®°å½•ï¼‰
        logger.info(f"ğŸ’¬ æ¶ˆæ¯å†…å®¹:")
        for i, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')[:200] + '...' if len(msg.get('content', '')) > 200 else msg.get('content', '')
            logger.info(f"  [{i+1}] {role}: {content}")
        
        try:
            # å‘é€APIè¯·æ±‚
            logger.info("â³ æ­£åœ¨è°ƒç”¨OpenAI API...")
            
            # æ„å»ºAPIè°ƒç”¨å‚æ•°
            api_params = {
                "model": model,
                "messages": messages,
                **kwargs
            }
            
            # å¦‚æœæ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ response_format
            if response_format and self._supports_structured_output(model):
                api_params["response_format"] = {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "code_review_response",
                        "schema": response_format
                    }
                }
                logger.info("ğŸ¯ ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼")
            elif response_format:
                logger.info("ğŸ“ æ¨¡å‹ä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œä½¿ç”¨æç¤ºè¯çº¦æŸ")
            
            response = await self.client.chat.completions.create(**api_params)
            
            # è®¡ç®—å“åº”æ—¶é—´
            end_time = time.time()
            response_time = end_time - start_time
            
            # è·å–å“åº”å†…å®¹
            response_content = response.choices[0].message.content
            response_tokens = len(response_content) // 4  # ç®€å•ä¼°ç®—
            
            # è®°å½•å“åº”è¯¦æƒ…
            logger.info("=" * 60)
            logger.info("âœ… OpenAI API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info(f"â±ï¸  å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            logger.info(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            
            # å°è¯•è·å–å®é™…tokenä½¿ç”¨é‡ï¼ˆå¦‚æœAPIè¿”å›äº†ï¼‰
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"  - è¾“å…¥tokens: {response.usage.prompt_tokens}")
                logger.info(f"  - è¾“å‡ºtokens: {response.usage.completion_tokens}")
                logger.info(f"  - æ€»tokens: {response.usage.total_tokens}")
            else:
                logger.info(f"  - ä¼°ç®—è¾“å…¥tokens: {input_tokens}")
                logger.info(f"  - ä¼°ç®—è¾“å‡ºtokens: {response_tokens}")
                
            logger.info(f"ğŸ¯ å“åº”å†…å®¹:")
            # è®°å½•å®Œæ•´å“åº”å†…å®¹ï¼Œä½†å¦‚æœå¤ªé•¿åˆ™æˆªå–
            if len(response_content) > 2000:
                logger.info(f"  {response_content[:800]}...")
                logger.info(f"  ... [ä¸­é—´çœç•¥ {len(response_content)-1600} ä¸ªå­—ç¬¦] ...")
                logger.info(f"  ...{response_content[-800:]}")
            else:
                logger.info(f"  {response_content}")
            
            # ä¸ºè°ƒè¯•ç›®çš„ï¼Œè®°å½•å“åº”çš„å‰1000ä¸ªå­—ç¬¦åˆ°DEBUGçº§åˆ«
            logger.debug(f"Full response preview: {response_content[:1000]}")
            
            logger.info("=" * 60)
            
            return response_content
            
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            logger.error("=" * 60)
            logger.error("âŒ OpenAI API è°ƒç”¨å¤±è´¥")
            logger.error("=" * 60)
            logger.error(f"â±ï¸  å¤±è´¥æ—¶é—´: {response_time:.2f}ç§’")
            logger.error(f"ğŸ’¥ é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error("=" * 60)
            raise
    
    async def close(self):
        """å®‰å…¨å…³é—­å®¢æˆ·ç«¯ï¼Œé¿å…çŠ¶æ€é”™è¯¯"""
        try:
            if hasattr(self, 'client') and self.client:
                # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æœ‰closeæ–¹æ³•ä¸”å¯ä»¥å®‰å…¨è°ƒç”¨
                if hasattr(self.client, 'close'):
                    await self.client.close()
                    logger.info("OpenAI client closed successfully")
        except Exception as e:
            # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯ï¼Œé¿å…å½±å“ä¸»è¦åŠŸèƒ½
            logger.warning(f"Error closing OpenAI client (ignored): {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            if hasattr(self, 'client') and self.client:
                # åœ¨åŒæ­¥ç¯å¢ƒä¸­æ— æ³•è°ƒç”¨å¼‚æ­¥closeï¼Œåªè®°å½•æ—¥å¿—
                logger.debug("OpenAI client cleanup in destructor")
        except:
            pass  # å¿½ç•¥ææ„å‡½æ•°ä¸­çš„æ‰€æœ‰é”™è¯¯
    
    def _supports_structured_output(self, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒç»“æ„åŒ–è¾“å‡º"""
        # é¦–å…ˆæ£€æŸ¥å…¨å±€é…ç½®
        if not settings.enable_structured_output:
            return False
        
        # å¦‚æœå¼ºåˆ¶å¯ç”¨ï¼Œç›´æ¥è¿”å›True
        if settings.force_structured_output:
            return True
        
        # OpenAI GPT-4 å’Œ GPT-3.5-turbo çš„è¾ƒæ–°ç‰ˆæœ¬æ”¯æŒç»“æ„åŒ–è¾“å‡º
        supported_models = [
            "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", 
            "gpt-4", "gpt-3.5-turbo"
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ¨¡å‹
        for supported in supported_models:
            if model.startswith(supported):
                return True
        
        # å¯¹äºè‡ªå®šä¹‰æ¨¡å‹ï¼Œæ ¹æ®é…ç½®å†³å®š
        return settings.force_structured_output

class SimpleAIProcessor:
    """ç®€åŒ–AIä»£ç åˆ†æå¤„ç†å™¨"""
    
    def __init__(self, model: str = None):
        self.model = model or settings.default_ai_model
        self.fallback_model = settings.fallback_ai_model
        self.token_manager = TokenManager(self.model)
        
        # å»¶è¿Ÿåˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œé¿å…å¯åŠ¨æ—¶çš„ä¾èµ–é—®é¢˜
        self._client = None
        
        logger.info(f"SimpleAIProcessor initialized with model: {self.model}")
    
    @property
    def client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œå¤±è´¥æ—¶è¿”å›Noneä»¥å¯ç”¨åŸºç¡€æ¨¡å¼"""
        if self._client is None:
            try:
                self._client = SimpleOpenAIClient(
                    api_key=settings.openai_api_key,
                    base_url=settings.api_base_url
                )
                logger.info("OpenAI client initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                logger.warning("AI client unavailable, will use basic analysis mode only")
                # è¿”å›Falseè¡¨ç¤ºå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                return False
        return self._client
    
    def _is_ai_available(self) -> bool:
        """æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not False
    
    async def analyze_merge_request(self, diff_files: List[FilePatchInfo], 
                                  review_type: str, mr_info: Dict) -> Dict[str, Any]:
        """åˆ†æMRçš„ä¸»å…¥å£å‡½æ•°"""
        import time
        
        # è®°å½•åˆ†æå¼€å§‹
        start_time = time.time()
        logger.info("ğŸ”" + "=" * 80)
        logger.info("ğŸ” å¼€å§‹ä»£ç å®¡æŸ¥åˆ†æ")
        logger.info("ğŸ”" + "=" * 80)
        logger.info(f"ğŸ“‹ å®¡æŸ¥å‚æ•°:")
        logger.info(f"  - å®¡æŸ¥ç±»å‹: {review_type}")
        logger.info(f"  - æ–‡ä»¶æ•°é‡: {len(diff_files)}")
        logger.info(f"  - AIæ¨¡å‹: {self.model}")
        logger.info(f"  - AIå®¢æˆ·ç«¯å¯ç”¨: {self._is_ai_available()}")
        logger.info(f"  - é€æ–‡ä»¶å®¡æŸ¥: {settings.enable_per_file_review}")
        
        logger.info(f"ğŸ“ å˜æ›´æ–‡ä»¶åˆ—è¡¨:")
        for i, file_patch in enumerate(diff_files):
            logger.info(f"  [{i+1}] {file_patch.filename} ({file_patch.edit_type})")
            
        logger.info(f"ğŸ¯ MRä¿¡æ¯:")
        logger.info(f"  - æ ‡é¢˜: {mr_info.get('title', 'æœªçŸ¥')}")
        logger.info(f"  - æºåˆ†æ”¯: {mr_info.get('source_branch', 'æœªçŸ¥')}")
        logger.info(f"  - ç›®æ ‡åˆ†æ”¯: {mr_info.get('target_branch', 'æœªçŸ¥')}")
        
        try:
            logger.info(f"âš¡ å¼€å§‹æ‰§è¡Œ {review_type} ç±»å‹å®¡æŸ¥...")
            
            # æ ¹æ®å®¡æŸ¥ç±»å‹å’Œé…ç½®é€‰æ‹©åˆ†æç­–ç•¥
            if settings.enable_per_file_review and len(diff_files) > 1:
                result = await self._per_file_analysis(diff_files, review_type, mr_info)
            elif review_type == "security":
                result = await self._security_focused_analysis(diff_files, mr_info)
            elif review_type == "performance":
                result = await self._performance_focused_analysis(diff_files, mr_info)
            elif review_type == "quick":
                result = await self._quick_analysis(diff_files, mr_info)
            else:  # full analysis
                result = await self._comprehensive_analysis(diff_files, mr_info)
            
            # ä¼°ç®—æˆæœ¬
            cost_estimate = self._estimate_analysis_cost(diff_files)
            result["cost_estimate"] = cost_estimate
            
            # è®°å½•åˆ†æå®Œæˆ
            end_time = time.time()
            analysis_time = end_time - start_time
            
            logger.info("âœ…" + "=" * 80)
            logger.info("âœ… ä»£ç å®¡æŸ¥åˆ†æå®Œæˆ")
            logger.info("âœ…" + "=" * 80)
            logger.info(f"â±ï¸  æ€»åˆ†ææ—¶é—´: {analysis_time:.2f}ç§’")
            logger.info(f"ğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ:")
            logger.info(f"  - å®¡æŸ¥ç±»å‹: {result.get('type', 'unknown')}")
            logger.info(f"  - è¯„åˆ†: {result.get('score', 0):.1f}/10.0")
            logger.info(f"  - å‘ç°é—®é¢˜æ•°: {len(result.get('findings', []))}")
            logger.info(f"  - å»ºè®®æ•°: {len(result.get('suggestions', []))}")
            logger.info(f"  - æ¨èæ•°: {len(result.get('recommendations', []))}")
            logger.info(f"  - æˆæœ¬ä¼°ç®—: ${cost_estimate:.4f}")
            
            if result.get('findings'):
                logger.info(f"ğŸ” å‘ç°çš„ä¸»è¦é—®é¢˜:")
                for i, finding in enumerate(result['findings'][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"  [{i+1}] {finding.get('filename', 'N/A')}: {finding.get('message', finding.get('description', 'N/A'))}")
                if len(result['findings']) > 3:
                    logger.info(f"  ... è¿˜æœ‰ {len(result['findings']) - 3} ä¸ªé—®é¢˜")
            
            logger.info(f"ğŸ“ æ€»ç»“: {result.get('summary', 'æ— æ€»ç»“')}")
            logger.info("âœ…" + "=" * 80)
            
            logger.info(f"Analysis completed with score: {result['score']}")
            return result
            
        except Exception as e:
            end_time = time.time()
            analysis_time = end_time - start_time
            
            logger.error("âŒ" + "=" * 80)
            logger.error("âŒ ä»£ç å®¡æŸ¥åˆ†æå¤±è´¥")
            logger.error("âŒ" + "=" * 80)
            logger.error(f"â±ï¸  å¤±è´¥æ—¶é—´: {analysis_time:.2f}ç§’")
            logger.error(f"ğŸ’¥ é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error("âŒ" + "=" * 80)
            logger.error(f"AI analysis failed: {e}")
            raise
        finally:
            # ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®æ¸…ç†
            await self._cleanup_client()
    
    async def _cleanup_client(self):
        """æ¸…ç†AIå®¢æˆ·ç«¯èµ„æº"""
        try:
            if self._client and self._client is not False:
                await self._client.close()
                logger.debug("AI client resources cleaned up")
        except Exception as e:
            # å¿½ç•¥æ¸…ç†é”™è¯¯ï¼Œé¿å…å½±å“ä¸»è¦æµç¨‹
            logger.debug(f"Client cleanup error (ignored): {e}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._cleanup_client()
    
    async def _per_file_analysis(self, diff_files: List[FilePatchInfo], 
                               review_type: str, mr_info: Dict) -> Dict[str, Any]:
        """é€æ–‡ä»¶å¹¶è¡Œåˆ†æ - æ–°çš„æ ¸å¿ƒæ–¹æ³•"""
        logger.info(f"ğŸš€ å¯åŠ¨é€æ–‡ä»¶å¹¶è¡Œåˆ†æï¼Œæ–‡ä»¶æ•°é‡: {len(diff_files)}")
        logger.info(f"ğŸ”§ æœ€å¤§å¹¶å‘æ•°: {settings.max_concurrent_file_reviews}")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(settings.max_concurrent_file_reviews)
        
        async def analyze_single_file(file_patch: FilePatchInfo) -> Dict[str, Any]:
            async with semaphore:
                return await self._analyze_single_file(file_patch, review_type)
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
        start_time = time.time()
        tasks = [analyze_single_file(file_patch) for file_patch in diff_files]
        file_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        parallel_time = time.time() - start_time
        logger.info(f"âš¡ å¹¶è¡Œæ–‡ä»¶åˆ†æå®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}ç§’")
        
        # èšåˆç»“æœ
        all_findings = []
        all_suggestions = []
        failed_files = []
        
        for i, result in enumerate(file_results):
            if isinstance(result, Exception):
                logger.error(f"æ–‡ä»¶ {diff_files[i].filename} åˆ†æå¤±è´¥: {result}")
                failed_files.append(diff_files[i].filename)
                continue
            
            all_findings.extend(result.get("findings", []))
            all_suggestions.extend(result.get("suggestions", []))
        
        # ç”Ÿæˆå…¨å±€æ€»ç»“
        global_summary = await self._generate_global_summary(
            all_findings, all_suggestions, mr_info, failed_files
        )
        
        # è®¡ç®—æ•´ä½“è¯„åˆ†
        score = self._calculate_overall_score(all_findings, diff_files, failed_files)
        
        logger.info(f"ğŸ“Š é€æ–‡ä»¶åˆ†æå®Œæˆ:")
        logger.info(f"  - æˆåŠŸåˆ†ææ–‡ä»¶: {len(diff_files) - len(failed_files)}")
        logger.info(f"  - å¤±è´¥æ–‡ä»¶: {len(failed_files)}")
        logger.info(f"  - æ€»é—®é¢˜æ•°: {len(all_findings)}")
        logger.info(f"  - æ€»å»ºè®®æ•°: {len(all_suggestions)}")
        
        return {
            "type": f"{review_type}_per_file",
            "findings": all_findings[:30],  # é™åˆ¶æ•°é‡é˜²æ­¢ç»“æœè¿‡å¤§
            "suggestions": all_suggestions[:20],
            "recommendations": self._generate_recommendations(all_findings),
            "score": score,
            "summary": global_summary,
            "failed_files": failed_files,
            "parallel_analysis_time": parallel_time
        }
    
    async def _analyze_single_file(self, file_patch: FilePatchInfo, 
                                 review_type: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        import time
        
        start_time = time.time()
        logger.info(f"ğŸ“„ å¼€å§‹åˆ†ææ–‡ä»¶: {file_patch.filename}")
        
        try:
            # åŸºç¡€é—®é¢˜æ£€æµ‹ï¼ˆæœ¬åœ°ï¼Œå¿«é€Ÿï¼‰
            basic_issues = self._detect_basic_issues(file_patch)
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶å†…å®¹ç”¨äºAIåˆ†æ
            full_file_content = self._prepare_file_content_for_analysis(file_patch)
            
            # AIæ·±åº¦åˆ†æ
            ai_findings = []
            ai_suggestions = []
            
            if self._is_ai_available() and full_file_content:
                try:
                    ai_result = await self._ai_single_file_analysis(
                        file_patch, full_file_content, review_type
                    )
                    ai_findings = ai_result.get("findings", [])
                    ai_suggestions = ai_result.get("suggestions", [])
                except Exception as e:
                    logger.warning(f"æ–‡ä»¶ {file_patch.filename} çš„AIåˆ†æå¤±è´¥: {e}")
            
            # åˆå¹¶ç»“æœ
            all_findings = basic_issues + ai_findings
            
            end_time = time.time()
            analysis_time = end_time - start_time
            
            logger.info(f"âœ… æ–‡ä»¶ {file_patch.filename} åˆ†æå®Œæˆ ({analysis_time:.2f}ç§’)")
            logger.info(f"    - åŸºç¡€é—®é¢˜: {len(basic_issues)}ä¸ª")
            logger.info(f"    - AIå‘ç°é—®é¢˜: {len(ai_findings)}ä¸ª") 
            logger.info(f"    - AIå»ºè®®: {len(ai_suggestions)}ä¸ª")
            
            return {
                "filename": file_patch.filename,
                "findings": all_findings,
                "suggestions": ai_suggestions,
                "analysis_time": analysis_time
            }
            
        except Exception as e:
            end_time = time.time()
            logger.error(f"âŒ æ–‡ä»¶ {file_patch.filename} åˆ†æå¤±è´¥: {e} ({end_time - start_time:.2f}ç§’)")
            raise
    
    def _prepare_file_content_for_analysis(self, file_patch: FilePatchInfo) -> str:
        """å‡†å¤‡ç”¨äºAIåˆ†æçš„æ–‡ä»¶å†…å®¹ï¼ˆåŒ…å«å®Œæ•´å†…å®¹å’Œdiffï¼‰"""
        logger.debug(f"å‡†å¤‡æ–‡ä»¶å†…å®¹: {file_patch.filename}")
        
        # è·å–æ–‡ä»¶å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨new_contentï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨old_contentï¼‰
        full_content = file_patch.new_content or file_patch.old_content
        
        if not full_content:
            logger.debug(f"æ–‡ä»¶ {file_patch.filename} æ— å®Œæ•´å†…å®¹ï¼Œä»…ä½¿ç”¨diff")
            return file_patch.patch
        
        # æŒ‰è¡Œæˆªæ–­æ–‡ä»¶å†…å®¹
        lines = full_content.splitlines()
        if len(lines) > settings.max_file_lines:
            logger.info(f"æ–‡ä»¶ {file_patch.filename} è¡Œæ•°è¿‡å¤š({len(lines)})ï¼Œæˆªæ–­è‡³{settings.max_file_lines}è¡Œ")
            truncated_content = '\n'.join(lines[:settings.max_file_lines])
            truncated_content += f"\n... [æ–‡ä»¶è¢«æˆªæ–­ï¼ŒåŸå§‹è¡Œæ•°: {len(lines)}]"
        else:
            truncated_content = full_content
        
        # ç»„åˆå®Œæ•´å†…å®¹å’Œå˜æ›´ä¿¡æ¯
        content_for_analysis = f"""
æ–‡ä»¶: {file_patch.filename}
å˜æ›´ç±»å‹: {file_patch.edit_type}

å®Œæ•´æ–‡ä»¶å†…å®¹:
```
{truncated_content}
```

å˜æ›´è¯¦æƒ…(diff):
```diff
{file_patch.patch}
```
"""
        return content_for_analysis.strip()
    
    async def _ai_single_file_analysis(self, file_patch: FilePatchInfo, 
                                     full_content: str, review_type: str) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡ŒAIåˆ†æ"""
        if not self._is_ai_available():
            return {"findings": [], "suggestions": []}
        
        # æ ¹æ®å®¡æŸ¥ç±»å‹æ„å»ºä¸åŒçš„æç¤ºè¯
        focus_areas = REVIEW_TYPES.get(review_type, {}).get("focus_areas", ["quality"])
        focus_description = ", ".join(focus_areas)
        
        base_prompt = f"""
è¯·ä¸“é—¨åˆ†æä»¥ä¸‹æ–‡ä»¶çš„ä»£ç è´¨é‡ï¼Œé‡ç‚¹å…³æ³¨: {focus_description}

{full_content}

è¯·æä¾›ï¼š
1. å‘ç°çš„å…·ä½“é—®é¢˜ï¼ŒåŒ…æ‹¬è¡Œå·å’Œè¯¦ç»†è¯´æ˜
2. é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®
3. è¯„ä¼°å˜æ›´çš„å½±å“å’Œé£é™©

æ³¨æ„ï¼šè¿™æ˜¯å•ç‹¬æ–‡ä»¶åˆ†æï¼Œè¯·ä¸“æ³¨äºè¯¥æ–‡ä»¶æœ¬èº«çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ•´ä½“æ¶æ„ã€‚
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{
    "findings": [
        {
            "type": "é—®é¢˜ç±»å‹",
            "line_number": è¡Œå·,
            "severity": "high/medium/low",
            "description": "é—®é¢˜æè¿°",
            "suggestion": "ä¿®å¤å»ºè®®"
        }
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}
"""
        else:
            prompt = base_prompt
        
        try:
            response = await self.client.chat_completion(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œä¸“æ³¨äºå•æ–‡ä»¶ä»£ç åˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                model=self.model,
                temperature=0.2,
                max_tokens=2000,
                response_format={
                    "type": "object",
                    "properties": {
                        "findings": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "type": {"type": "string"},
                                    "line_number": {"type": "integer"},
                                    "severity": {"type": "string", "enum": ["high", "medium", "low"]},
                                    "description": {"type": "string"},
                                    "suggestion": {"type": "string"}
                                },
                                "required": ["type", "severity", "description"]
                            }
                        },
                        "suggestions": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    },
                    "required": ["findings", "suggestions"]
                }
            )
            
            # è§£æå“åº”
            cleaned_response = self._extract_json_from_response(response)
            result = json.loads(cleaned_response)
            
            # ä¸ºæ¯ä¸ªfindingæ·»åŠ filename
            for finding in result.get("findings", []):
                finding["filename"] = file_patch.filename
            
            return result
            
        except Exception as e:
            logger.error(f"AIå•æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return {"findings": [], "suggestions": []}
    
    async def _generate_global_summary(self, all_findings: List[Dict], 
                                     all_suggestions: List[str], mr_info: Dict,
                                     failed_files: List[str]) -> str:
        """ç”Ÿæˆå…¨å±€åˆ†ææ€»ç»“"""
        if not self._is_ai_available():
            high_issues = len([f for f in all_findings if f.get("severity") == "high"])
            medium_issues = len([f for f in all_findings if f.get("severity") == "medium"])
            return f"é€æ–‡ä»¶åˆ†æå®Œæˆï¼Œå‘ç°{high_issues}ä¸ªé«˜é£é™©é—®é¢˜ï¼Œ{medium_issues}ä¸ªä¸­ç­‰é£é™©é—®é¢˜ã€‚"
        
        # ç»Ÿè®¡ä¿¡æ¯
        high_issues = len([f for f in all_findings if f.get("severity") == "high"])
        medium_issues = len([f for f in all_findings if f.get("severity") == "medium"])
        low_issues = len([f for f in all_findings if f.get("severity") == "low"])
        
        # é—®é¢˜åˆ†ç±»ç»Ÿè®¡
        issue_types = {}
        for finding in all_findings:
            issue_type = finding.get("type", "unknown")
            issue_types[issue_type] = issue_types.get(issue_type, 0) + 1
        
        top_issues = sorted(issue_types.items(), key=lambda x: x[1], reverse=True)[:5]
        
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹é€æ–‡ä»¶ä»£ç å®¡æŸ¥ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…¨å±€æ€»ç»“ï¼š

MRä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- æºåˆ†æ”¯ï¼š{mr_info.get('source_branch', 'æœªçŸ¥')}
- ç›®æ ‡åˆ†æ”¯ï¼š{mr_info.get('target_branch', 'æœªçŸ¥')}

åˆ†æç»Ÿè®¡ï¼š
- é«˜é£é™©é—®é¢˜ï¼š{high_issues}ä¸ª
- ä¸­ç­‰é£é™©é—®é¢˜ï¼š{medium_issues}ä¸ª
- ä½é£é™©é—®é¢˜ï¼š{low_issues}ä¸ª
- åˆ†æå¤±è´¥æ–‡ä»¶ï¼š{len(failed_files)}ä¸ª

ä¸»è¦é—®é¢˜ç±»å‹ï¼š
{chr(10).join([f"- {issue_type}: {count}ä¸ª" for issue_type, count in top_issues])}

æ”¹è¿›å»ºè®®æ•°é‡ï¼š{len(all_suggestions)}ä¸ª

è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œè¯„ä¼°æ•´ä½“ä»£ç è´¨é‡å’Œä¸»è¦æ”¹è¿›æ–¹å‘ã€‚
"""
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.3,
                max_tokens=300
            )
            return response.strip()
        except Exception as e:
            logger.error(f"ç”Ÿæˆå…¨å±€æ€»ç»“å¤±è´¥: {e}")
            return f"é€æ–‡ä»¶å¹¶è¡Œåˆ†æå®Œæˆï¼Œæ€»å…±å‘ç°{len(all_findings)}ä¸ªé—®é¢˜ï¼Œ{len(all_suggestions)}ä¸ªå»ºè®®ã€‚"
    
    def _calculate_overall_score(self, all_findings: List[Dict], 
                               diff_files: List[FilePatchInfo], 
                               failed_files: List[str]) -> float:
        """è®¡ç®—æ•´ä½“è¯„åˆ†"""
        base_score = 8.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for finding in all_findings:
            severity = finding.get("severity", "low")
            if severity == "high":
                base_score -= 1.0
            elif severity == "medium":
                base_score -= 0.5
            else:
                base_score -= 0.2
        
        # æ ¹æ®å¤±è´¥æ–‡ä»¶æ‰£åˆ†
        if failed_files:
            failure_penalty = len(failed_files) * 0.5
            base_score -= failure_penalty
            logger.info(f"ç”±äº{len(failed_files)}ä¸ªæ–‡ä»¶åˆ†æå¤±è´¥ï¼Œæ‰£åˆ†{failure_penalty}")
        
        # æ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´
        if len(diff_files) > 10:
            base_score -= 0.3
        
        return max(base_score, 2.0)  # æœ€ä½2åˆ†
    
    async def _comprehensive_analysis(self, diff_files: List[FilePatchInfo], 
                                    mr_info: Dict) -> Dict[str, Any]:
        """å…¨é¢åˆ†æ"""
        all_findings = []
        all_suggestions = []
        
        # åŸºç¡€é—®é¢˜æ£€æµ‹
        for file_patch in diff_files:
            basic_issues = self._detect_basic_issues(file_patch)
            all_findings.extend(basic_issues)
        
        # AIæ·±åº¦åˆ†æ
        try:
            ai_analysis = await self._ai_comprehensive_analysis(diff_files, mr_info)
            all_findings.extend(ai_analysis.get("findings", []))
            all_suggestions.extend(ai_analysis.get("suggestions", []))
        except Exception as e:
            logger.warning(f"AI analysis failed, using basic analysis only: {e}")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        score = self._calculate_comprehensive_score(all_findings, diff_files)
        
        return {
            "type": "comprehensive",
            "findings": all_findings[:20],  # é™åˆ¶æ•°é‡
            "suggestions": all_suggestions[:10],
            "recommendations": self._generate_recommendations(all_findings),
            "score": score,
            "summary": self._generate_summary(all_findings, score, len(diff_files))
        }
    
    async def _security_focused_analysis(self, diff_files: List[FilePatchInfo], 
                                       mr_info: Dict) -> Dict[str, Any]:
        """å®‰å…¨ä¸“é¡¹åˆ†æ"""
        security_issues = []
        
        for file_patch in diff_files:
            # æ£€æµ‹å¸¸è§å®‰å…¨é—®é¢˜
            issues = self._detect_security_issues(file_patch)
            security_issues.extend(issues)
            
            # å¯¹æ¯ä¸ªæ½œåœ¨é—®é¢˜è¿›è¡ŒAIæ·±åº¦åˆ†æ
            for issue in issues:
                try:
                    ai_result = await self._ai_security_analysis(
                        file_patch.filename, 
                        issue["line_number"], 
                        issue["code_line"],
                        issue["category"]
                    )
                    if ai_result["is_vulnerability"]:
                        security_issues.append(ai_result)
                except Exception as e:
                    logger.warning(f"Security AI analysis failed: {e}")
        
        security_score = self._calculate_security_score(security_issues)
        
        return {
            "type": "security",
            "findings": security_issues,
            "score": security_score,
            "summary": f"å‘ç° {len(security_issues)} ä¸ªå®‰å…¨é—®é¢˜",
            "recommendations": self._generate_security_recommendations(security_issues)
        }
    
    async def _performance_focused_analysis(self, diff_files: List[FilePatchInfo], 
                                          mr_info: Dict) -> Dict[str, Any]:
        """æ€§èƒ½ä¸“é¡¹åˆ†æ"""
        performance_issues = []
        
        for file_patch in diff_files:
            # æ£€æŸ¥å¸¸è§æ€§èƒ½é—®é¢˜
            issues = self._detect_performance_issues(file_patch)
            performance_issues.extend(issues)
        
        # ä½¿ç”¨AIè¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ
        if performance_issues:
            try:
                ai_analysis = await self._ai_performance_analysis(performance_issues, diff_files)
                performance_issues.extend(ai_analysis)
            except Exception as e:
                logger.warning(f"Performance AI analysis failed: {e}")
        
        performance_score = self._calculate_performance_score(performance_issues)
        
        return {
            "type": "performance",
            "findings": performance_issues,
            "score": performance_score,
            "summary": f"å‘ç° {len(performance_issues)} ä¸ªæ€§èƒ½é—®é¢˜",
            "recommendations": self._generate_performance_recommendations(performance_issues)
        }
    
    async def _quick_analysis(self, diff_files: List[FilePatchInfo], 
                            mr_info: Dict) -> Dict[str, Any]:
        """å¿«é€ŸåŸºç¡€åˆ†æ"""
        basic_issues = []
        
        for file_patch in diff_files:
            issues = self._detect_basic_issues(file_patch)
            basic_issues.extend(issues)
        
        # å¿«é€ŸAIæ€»ç»“
        try:
            quick_summary = await self._ai_quick_summary(diff_files, mr_info)
        except Exception as e:
            logger.warning(f"Quick AI summary failed: {e}")
            quick_summary = "å¿«é€Ÿåˆ†æå®Œæˆï¼Œå‘ç°åŸºç¡€é—®é¢˜ã€‚"
        
        score = 8.0 - min(len(basic_issues) * 0.5, 3.0)  # åŸºç¡€è¯„åˆ†é€»è¾‘
        
        return {
            "type": "quick",
            "findings": basic_issues[:10],  # é™åˆ¶æ•°é‡
            "score": max(score, 5.0),
            "summary": quick_summary,
            "recommendations": ["å»ºè®®è¿›è¡Œå®Œæ•´å®¡æŸ¥ä»¥å‘ç°æ›´å¤šé—®é¢˜"]
        }
    
    def _detect_basic_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹åŸºç¡€ä»£ç é—®é¢˜"""
        logger.info(f"ğŸ” å¼€å§‹åŸºç¡€é—®é¢˜æ£€æµ‹: {file_patch.filename}")
        
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        logger.info(f"  - æ–‡ä»¶ç±»å‹: {file_patch.edit_type}")
        logger.info(f"  - æ–°å¢è¡Œæ•°: {len(new_lines)}")
        
        detected_issues_by_type = {}
        
        for line_num, line in enumerate(new_lines, 1):
            line_content = line[1:].strip()  # ç§»é™¤'+'ç¬¦å·
            
            # æ£€æŸ¥åŸºç¡€é—®é¢˜
            if len(line_content) > 120:
                issue = {
                    "type": "line_too_long",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "message": "ä»£ç è¡Œè¿‡é•¿",
                    "severity": "low"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("line_too_long", 0)
                detected_issues_by_type["line_too_long"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: ä»£ç è¡Œè¿‡é•¿ ({len(line_content)} å­—ç¬¦)")
            
            if re.search(r"console\.log|print\(.*\)|System\.out\.println", line_content):
                issue = {
                    "type": "debug_statement",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "message": "å¯èƒ½åŒ…å«è°ƒè¯•è¯­å¥",
                    "severity": "medium"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("debug_statement", 0)
                detected_issues_by_type["debug_statement"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: æ£€æµ‹åˆ°è°ƒè¯•è¯­å¥: {line_content[:50]}...")
            
            if re.search(r"TODO|FIXME|HACK", line_content, re.IGNORECASE):
                issue = {
                    "type": "todo_comment",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "message": "åŒ…å«å¾…åŠäº‹é¡¹æˆ–ä¿®å¤æ ‡è®°",
                    "severity": "low"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("todo_comment", 0)
                detected_issues_by_type["todo_comment"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: å‘ç°TODO/FIXME: {line_content[:50]}...")
        
        # æ€»ç»“æ£€æµ‹ç»“æœ
        if issues:
            logger.info(f"  ğŸ“Š {file_patch.filename} æ£€æµ‹ç»“æœ:")
            for issue_type, count in detected_issues_by_type.items():
                logger.info(f"    - {issue_type}: {count}ä¸ª")
            logger.info(f"  âœ… æ€»è®¡å‘ç° {len(issues)} ä¸ªåŸºç¡€é—®é¢˜")
        else:
            logger.info(f"  âœ… {file_patch.filename} æœªå‘ç°åŸºç¡€é—®é¢˜")
        
        return issues
    
    def _detect_security_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹å®‰å…¨é—®é¢˜"""
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        security_patterns = {
            "sql_injection": [r"execute\(.*\+.*\)", r"query\(.*\+.*\)", r"SELECT.*\+"],
            "xss": [r"innerHTML\s*=", r"document\.write\(", r"eval\("],
            "hardcoded_secrets": [r"password\s*=\s*['\"]", r"api[_-]?key\s*=\s*['\"]", r"secret\s*=\s*['\"]"],
            "path_traversal": [r"\.\.\/", r"\.\.\\\\", r"os\.path\.join.*\.\."],
            "command_injection": [r"os\.system\(", r"subprocess\.", r"exec\(", r"shell=True"]
        }
        
        for line_num, line in enumerate(new_lines, 1):
            for category, patterns in security_patterns.items():
                if any(re.search(pattern, line, re.IGNORECASE) for pattern in patterns):
                    issues.append({
                        "type": "potential_security_issue",
                        "category": category,
                        "filename": file_patch.filename,
                        "line_number": line_num,
                        "code_line": line.strip(),
                        "severity": "high"
                    })
        
        return issues
    
    def _detect_performance_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹æ€§èƒ½é—®é¢˜"""
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        performance_patterns = {
            "n_plus_1_query": [r"for.*in.*:", r"\.get\(", r"\.filter\("],
            "inefficient_loop": [r"for.*in.*for.*in", r"while.*while"],
            "memory_leak": [r"\.append\(", r"global\s+", r"cache\["],
            "blocking_io": [r"requests\.get", r"urllib\.request", r"time\.sleep"],
            "inefficient_data_structure": [r"list\(\)", r"dict\(\)", r"\[\].*in.*for"]
        }
        
        for line_num, line in enumerate(new_lines, 1):
            for issue_type, patterns in performance_patterns.items():
                if any(re.search(pattern, line, re.IGNORECASE) for pattern in patterns):
                    issues.append({
                        "type": issue_type,
                        "filename": file_patch.filename,
                        "line_number": line_num,
                        "code_line": line.strip(),
                        "severity": "medium",
                        "suggestion": self._get_performance_suggestion(issue_type)
                    })
        
        return issues
    
    async def _ai_comprehensive_analysis(self, diff_files: List[FilePatchInfo], mr_info: Dict) -> Dict[str, Any]:
        """AIç»¼åˆåˆ†æ"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI comprehensive analysis")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡AIåˆ†æ"}
        
        # æ„å»ºåˆ†ææç¤ºè¯
        file_summaries = []
        for file_patch in diff_files[:10]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
            summary = f"æ–‡ä»¶: {file_patch.filename}\n"
            summary += f"å˜æ›´ç±»å‹: {file_patch.edit_type}\n"
            summary += f"å˜æ›´å†…å®¹:\n{file_patch.patch[:1000]}...\n"  # é™åˆ¶é•¿åº¦
            file_summaries.append(summary)
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
è¯·åˆ†æä»¥ä¸‹GitLab Merge Requestçš„ä»£ç å˜æ›´ï¼š

MRä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- æºåˆ†æ”¯ï¼š{mr_info.get('source_branch', 'æœªçŸ¥')}
- ç›®æ ‡åˆ†æ”¯ï¼š{mr_info.get('target_branch', 'æœªçŸ¥')}

æ–‡ä»¶å˜æ›´ï¼š
{chr(10).join(file_summaries)}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µ
2. æ½œåœ¨çš„bugå’Œé—®é¢˜
3. æ€§èƒ½å½±å“
4. å®‰å…¨è€ƒè™‘
5. å¯ç»´æŠ¤æ€§
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{
    "findings": [
        {
            "type": "é—®é¢˜ç±»å‹",
            "filename": "æ–‡ä»¶å", 
            "line_number": è¡Œå·,
            "severity": "high/medium/low",
            "description": "é—®é¢˜æè¿°",
            "suggestion": "ä¿®å¤å»ºè®®"
        }
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
    "overall_assessment": "æ•´ä½“è¯„ä¼°"
}
"""
        else:
            prompt = base_prompt + "\nè¯·æä¾›è¯¦ç»†çš„ä»£ç å®¡æŸ¥åˆ†æã€‚"
        
        try:
            response = await self.client.chat_completion(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·ä»”ç»†åˆ†æä»£ç å¹¶æä¾›å»ºè®¾æ€§çš„åé¦ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                model=self.model,
                temperature=0.2,
                max_tokens=4000,
                response_format=CODE_REVIEW_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            logger.warning(f"AI response is not valid JSON: {e}")
            logger.debug(f"Raw response: {response[:500]}...")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIåˆ†æå¤±è´¥"}
        except Exception as e:
            logger.error(f"Error processing AI response: {e}")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIåˆ†æå¤„ç†å¤±è´¥"}
    
    def _extract_json_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        response = response.strip()
        
        # å¦‚æœå“åº”åŒ…å«```jsonæ ‡è®°ï¼Œæå–å…¶ä¸­çš„JSON
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            if end != -1:
                json_content = response[start:end].strip()
                logger.debug("Extracted JSON from markdown block")
                return json_content
        
        # å¦‚æœå“åº”åŒ…å«```æ ‡è®°ä½†æ²¡æœ‰jsonæ ‡è¯†ï¼Œä¹Ÿå°è¯•æå–
        if response.startswith("```") and response.endswith("```"):
            lines = response.split('\n')
            if len(lines) > 2:
                json_content = '\n'.join(lines[1:-1]).strip()
                logger.debug("Extracted content from code block")
                return json_content
        
        # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
        start_idx = response.find('{')
        if start_idx != -1:
            # ä»ç¬¬ä¸€ä¸ª{å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„}
            brace_count = 0
            for i, char in enumerate(response[start_idx:], start_idx):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_content = response[start_idx:i+1]
                        logger.debug("Extracted JSON object from response")
                        return json_content
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONç»“æ„ï¼Œè¿”å›åŸå§‹å“åº”
        logger.debug("No JSON structure found, returning original response")
        return response
    
    async def _ai_security_analysis(self, filename: str, line_num: int, code_line: str, category: str) -> Dict[str, Any]:
        """AIå®‰å…¨åˆ†æ"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI security analysis")
            return {
                "is_vulnerability": False,
                "risk_level": 0,
                "description": "AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒAIå®‰å…¨åˆ†æ",
                "filename": filename,
                "line_number": line_num,
                "category": category
            }
            
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
åˆ†æä»¥ä¸‹ä»£ç è¡Œæ˜¯å¦å­˜åœ¨å®‰å…¨æ¼æ´ï¼š

æ–‡ä»¶ï¼š{filename}
è¡Œå·ï¼š{line_num}  
ä»£ç ï¼š{code_line}
å¯èƒ½é—®é¢˜ï¼š{category}

è¯·åˆ†æï¼š
1. è¿™æ˜¯å¦çœŸçš„æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Ÿ
2. é£é™©çº§åˆ«ï¼ˆ1-10ï¼‰
3. å…·ä½“çš„å®‰å…¨é£é™©
4. ä¿®å¤å»ºè®®
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{
    "is_vulnerability": true/false,
    "risk_level": 1-10,
    "description": "è¯¦ç»†æè¿°",
    "fix_suggestion": "ä¿®å¤å»ºè®®"
}
"""
        else:
            prompt = base_prompt
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.1,
                max_tokens=500,
                response_format=SECURITY_ANALYSIS_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            result = json.loads(cleaned_response)
            result.update({
                "filename": filename,
                "line_number": line_num,
                "code_line": code_line,
                "category": category
            })
            return result
            
        except Exception as e:
            logger.error(f"Failed to analyze security issue: {e}")
            return {
                "is_vulnerability": False,
                "risk_level": 0,
                "description": "åˆ†æå¤±è´¥",
                "filename": filename,
                "line_number": line_num,
                "category": category
            }
    
    async def _ai_performance_analysis(self, issues: List[Dict], diff_files: List[FilePatchInfo]) -> List[Dict]:
        """AIæ€§èƒ½åˆ†æ"""
        if not issues:
            return []
            
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI performance analysis")
            return []
        
        # æ„å»ºæ€§èƒ½åˆ†ææç¤º
        issues_summary = "\n".join([
            f"- {issue['type']} in {issue['filename']}:{issue['line_number']}"
            for issue in issues[:5]
        ])
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
æ£€æµ‹åˆ°ä»¥ä¸‹æ€§èƒ½é—®é¢˜ï¼š
{issues_summary}

è¯·åˆ†æè¿™äº›é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼å›å¤ï¼Œæ¯ä¸ªé—®é¢˜ä¸€ä¸ªå¯¹è±¡ï¼š
[
    {
        "type": "æ€§èƒ½é—®é¢˜ç±»å‹",
        "severity": "high/medium/low", 
        "description": "é—®é¢˜æè¿°",
        "optimization": "ä¼˜åŒ–å»ºè®®"
    }
]
"""
        else:
            prompt = base_prompt
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.2,
                max_tokens=1000,
                response_format=PERFORMANCE_ANALYSIS_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            return json.loads(cleaned_response)
        except Exception as e:
            logger.error(f"Performance analysis failed: {e}")
            return []
    
    async def _ai_quick_summary(self, diff_files: List[FilePatchInfo], mr_info: Dict) -> str:
        """AIå¿«é€Ÿæ€»ç»“"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, using basic summary")
            files_count = len(diff_files)
            return f"å¿«é€Ÿåˆ†æå®Œæˆï¼Œå˜æ›´æ¶‰åŠ{files_count}ä¸ªæ–‡ä»¶ã€‚AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä»…æä¾›åŸºç¡€åˆ†æã€‚"
            
        files_info = ", ".join([f.filename for f in diff_files[:5]])
        if len(diff_files) > 5:
            files_info += f" ç­‰{len(diff_files)}ä¸ªæ–‡ä»¶"
        
        prompt = f"""
å¿«é€Ÿæ€»ç»“ä»¥ä¸‹MRçš„å˜æ›´ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- å˜æ›´æ–‡ä»¶ï¼š{files_info}
- æ€»æ–‡ä»¶æ•°ï¼š{len(diff_files)}

è¯·ç”¨1-2å¥è¯æ€»ç»“ä¸»è¦å˜æ›´å†…å®¹å’Œå½±å“ã€‚
"""
        
        try:
            return await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.3,
                max_tokens=200
            )
        except Exception as e:
            logger.error(f"Quick summary failed: {e}")
            return f"å¿«é€Ÿåˆ†æå®Œæˆï¼Œå˜æ›´æ¶‰åŠ{len(diff_files)}ä¸ªæ–‡ä»¶ã€‚"
    
    def _calculate_comprehensive_score(self, findings: List[Dict], diff_files: List[FilePatchInfo]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        base_score = 8.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for finding in findings:
            severity = finding.get("severity", "low")
            if severity == "high":
                base_score -= 1.0
            elif severity == "medium":
                base_score -= 0.5
            else:
                base_score -= 0.2
        
        # æ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´
        if len(diff_files) > 10:
            base_score -= 0.5
        
        return max(base_score, 3.0)  # æœ€ä½3åˆ†
    
    def _calculate_security_score(self, security_issues: List[Dict]) -> float:
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        base_score = 9.0
        
        for issue in security_issues:
            risk_level = issue.get("risk_level", 0)
            base_score -= risk_level * 0.3
        
        return max(base_score, 1.0)
    
    def _calculate_performance_score(self, performance_issues: List[Dict]) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        base_score = 8.0
        
        for issue in performance_issues:
            severity = issue.get("severity", "low")
            if severity == "high":
                base_score -= 1.5
            elif severity == "medium":
                base_score -= 0.8
            else:
                base_score -= 0.3
        
        return max(base_score, 2.0)
    
    def _generate_summary(self, findings: List[Dict], score: float, files_count: int) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        high_issues = len([f for f in findings if f.get("severity") == "high"])
        medium_issues = len([f for f in findings if f.get("severity") == "medium"])
        
        if score >= 8.0:
            quality = "ä¼˜ç§€"
        elif score >= 6.0:
            quality = "è‰¯å¥½"
        elif score >= 4.0:
            quality = "ä¸€èˆ¬"
        else:
            quality = "éœ€è¦æ”¹è¿›"
        
        return f"ä»£ç è´¨é‡è¯„ä¼°ï¼š{quality}ã€‚åˆ†æäº†{files_count}ä¸ªæ–‡ä»¶ï¼Œå‘ç°{high_issues}ä¸ªé«˜é£é™©é—®é¢˜ï¼Œ{medium_issues}ä¸ªä¸­ç­‰é£é™©é—®é¢˜ã€‚"
    
    def _generate_recommendations(self, findings: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if any(f.get("type") == "debug_statement" for f in findings):
            recommendations.append("ç§»é™¤è°ƒè¯•è¯­å¥å’Œæ—¥å¿—è¾“å‡º")
        
        if any(f.get("type") == "line_too_long" for f in findings):
            recommendations.append("ä¿æŒä»£ç è¡Œé•¿åº¦åœ¨åˆç†èŒƒå›´å†…")
        
        if any(f.get("severity") == "high" for f in findings):
            recommendations.append("ä¼˜å…ˆå¤„ç†é«˜é£é™©é—®é¢˜")
        
        recommendations.append("å»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–å˜æ›´ä»£ç ")
        recommendations.append("ç¡®ä¿ä»£ç éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒ")
        
        return recommendations
    
    def _generate_security_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        recommendations = []
        
        categories = set(issue.get("category") for issue in issues)
        
        if "sql_injection" in categories:
            recommendations.append("ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥")
        if "xss" in categories:
            recommendations.append("å¯¹è¾“å‡ºå†…å®¹è¿›è¡Œé€‚å½“è½¬ä¹‰")
        if "hardcoded_secrets" in categories:
            recommendations.append("å°†æ•æ„Ÿä¿¡æ¯å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­")
        if "command_injection" in categories:
            recommendations.append("é¿å…ç›´æ¥æ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‘½ä»¤")
        
        return recommendations
    
    def _generate_performance_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []
        
        types = set(issue.get("type") for issue in issues)
        
        if "n_plus_1_query" in types:
            recommendations.append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œé¿å…N+1é—®é¢˜")
        if "inefficient_loop" in types:
            recommendations.append("ä¼˜åŒ–å¾ªç¯é€»è¾‘ï¼Œå‡å°‘åµŒå¥—å±‚çº§")
        if "blocking_io" in types:
            recommendations.append("è€ƒè™‘ä½¿ç”¨å¼‚æ­¥I/Oæ“ä½œ")
        if "memory_leak" in types:
            recommendations.append("æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼")
        
        return recommendations
    
    def _get_performance_suggestion(self, issue_type: str) -> str:
        """è·å–æ€§èƒ½é—®é¢˜å»ºè®®"""
        suggestions = {
            "n_plus_1_query": "è€ƒè™‘ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢æˆ–é¢„åŠ è½½",
            "inefficient_loop": "å°è¯•ä¼˜åŒ–ç®—æ³•æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„",
            "memory_leak": "ç¡®ä¿åŠæ—¶æ¸…ç†ä¸éœ€è¦çš„å¯¹è±¡å¼•ç”¨",
            "blocking_io": "ä½¿ç”¨å¼‚æ­¥æ“ä½œæˆ–çº¿ç¨‹æ± ",
            "inefficient_data_structure": "é€‰æ‹©æ›´é€‚åˆçš„æ•°æ®ç»“æ„"
        }
        return suggestions.get(issue_type, "è€ƒè™‘ä¼˜åŒ–æ­¤å¤„çš„æ€§èƒ½")
    
    def _estimate_analysis_cost(self, diff_files: List[FilePatchInfo]) -> float:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        total_tokens = 0
        for file_patch in diff_files:
            total_tokens += self.token_manager.count_tokens(file_patch.patch)
        
        return self.token_manager.estimate_cost(total_tokens)
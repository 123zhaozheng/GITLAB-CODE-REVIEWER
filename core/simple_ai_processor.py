"""
ç®€åŒ–AIå¤„ç†å™¨æ¨¡å—
ç›´æ¥ä½¿ç”¨OpenAI APIï¼Œé¿å…LiteLLMçš„å¤æ‚æ€§
"""
import asyncio
import json
import re
import time
from typing import Dict, List, Optional, Any
# import tiktoken  # å·²ç§»é™¤ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—æ›¿ä»£
import openai
import logging

from config.settings import settings, MODEL_COSTS, REVIEW_TYPES
from core.gitlab_client import FilePatchInfo

logger = logging.getLogger(__name__)

# JSON Schemaå®šä¹‰ - ç”¨äºç»“æ„åŒ–è¾“å‡º
CODE_REVIEW_SCHEMA = {
    "type": "object",
    "properties": {
        "findings": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "type": {"type": "string", "description": "é—®é¢˜ç±»å‹"},
                    "filename": {"type": "string", "description": "æ–‡ä»¶å"},
                    "line_number": {"type": "integer", "description": "è¡Œå·"},
                    "severity": {
                        "type": "string", 
                        "enum": ["high", "medium", "low"],
                        "description": "ä¸¥é‡ç¨‹åº¦"
                    },
                    "description": {"type": "string", "description": "é—®é¢˜æè¿°"},
                    "suggestion": {"type": "string", "description": "ä¿®å¤å»ºè®®"}
                },
                "required": ["type", "filename", "severity", "description"]
            }
        },
        "suggestions": {
            "type": "array",
            "items": {"type": "string"},
            "description": "æ”¹è¿›å»ºè®®åˆ—è¡¨"
        },
        "overall_assessment": {
            "type": "string",
            "description": "æ•´ä½“è¯„ä¼°"
        }
    },
    "required": ["findings", "suggestions", "overall_assessment"]
}

SECURITY_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "is_vulnerability": {"type": "boolean", "description": "æ˜¯å¦ä¸ºå®‰å…¨æ¼æ´"},
        "risk_level": {
            "type": "integer", 
            "minimum": 1, 
            "maximum": 10,
            "description": "é£é™©çº§åˆ«(1-10)"
        },
        "description": {"type": "string", "description": "è¯¦ç»†æè¿°"},
        "fix_suggestion": {"type": "string", "description": "ä¿®å¤å»ºè®®"}
    },
    "required": ["is_vulnerability", "risk_level", "description", "fix_suggestion"]
}

PERFORMANCE_ANALYSIS_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "type": {"type": "string", "description": "æ€§èƒ½é—®é¢˜ç±»å‹"},
            "severity": {
                "type": "string",
                "enum": ["high", "medium", "low"],
                "description": "ä¸¥é‡ç¨‹åº¦"
            },
            "description": {"type": "string", "description": "é—®é¢˜æè¿°"},
            "optimization": {"type": "string", "description": "ä¼˜åŒ–å»ºè®®"}
        },
        "required": ["type", "severity", "description", "optimization"]
    }
}

class TokenManager:
    """Tokenç®¡ç†å™¨ - ä½¿ç”¨ç®€å•ä¼°ç®—ï¼Œä¸ä¾èµ–tiktoken"""
    
    # ä¸åŒæ¨¡å‹çš„å­—ç¬¦-tokenæ¯”ä¾‹ (å­—ç¬¦æ•°/tokenæ•°)
    CHAR_TOKEN_RATIOS = {
        'gpt-4': 3.8,
        'gpt-4-turbo': 3.8, 
        'gpt-3.5-turbo': 3.8,
        'claude': 4.0,
        'llama': 3.5,      # Llama ç³»åˆ—
        'qwen': 4.2,       # é€šä¹‰åƒé—® (ä¸­æ–‡å‹å¥½)
        'chatglm': 4.2,    # ChatGLM (ä¸­æ–‡å‹å¥½)
        'deepseek': 3.8,   # DeepSeek
        'default': 4.0     # é»˜è®¤æ¯”ä¾‹
    }
    
    def __init__(self, model: str = None):
        self.model = model or settings.default_ai_model
        self.ratio = self._get_ratio_for_model(self.model)
        logger.info(f"TokenManager initialized for model '{self.model}' with ratio {self.ratio}")
    
    def _get_ratio_for_model(self, model: str) -> float:
        """æ ¹æ®æ¨¡å‹åè·å–å­—ç¬¦-tokenæ¯”ä¾‹"""
        model_lower = model.lower()
        
        # æ£€æŸ¥æ¨¡å‹åä¸­æ˜¯å¦åŒ…å«å·²çŸ¥æ¨¡å‹ç±»å‹
        for model_type, ratio in self.CHAR_TOKEN_RATIOS.items():
            if model_type in model_lower:
                return ratio
        
        # å¦‚æœæ˜¯ä¸­æ–‡ç›¸å…³çš„æ¨¡å‹ï¼Œä½¿ç”¨æ›´é«˜çš„æ¯”ä¾‹
        chinese_indicators = ['chinese', 'zh', 'cn', 'baichuan', 'internlm']
        if any(indicator in model_lower for indicator in chinese_indicators):
            return 4.5
            
        return self.CHAR_TOKEN_RATIOS['default']
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬tokenæ•°é‡ - åŸºäºå­—ç¬¦æ•°çš„ç®€å•ä¼°ç®—"""
        if not text:
            return 0
        
        char_count = len(text)
        
        # æ£€æµ‹æ–‡æœ¬ç±»å‹å¹¶è°ƒæ•´æ¯”ä¾‹
        adjusted_ratio = self._adjust_ratio_for_content(text, self.ratio)
        
        # è®¡ç®—tokenæ•°é‡
        estimated_tokens = max(1, int(char_count / adjusted_ratio))
        
        return estimated_tokens
    
    def _adjust_ratio_for_content(self, text: str, base_ratio: float) -> float:
        """æ ¹æ®æ–‡æœ¬å†…å®¹è°ƒæ•´å­—ç¬¦-tokenæ¯”ä¾‹"""
        # æ£€æµ‹æ˜¯å¦æ˜¯ä»£ç æ–‡æœ¬
        code_patterns = [
            r'def\s+\w+\s*\(',      # Pythonå‡½æ•°å®šä¹‰
            r'class\s+\w+',         # ç±»å®šä¹‰
            r'function\s*\(',       # JavaScriptå‡½æ•°
            r'import\s+\w+',        # å¯¼å…¥è¯­å¥
            r'from\s+\w+\s+import', # Pythonå¯¼å…¥
            r'{\s*$',               # ä»£ç å—å¼€å§‹
            r'}\s*$',               # ä»£ç å—ç»“æŸ
            r'if\s*\(',             # æ¡ä»¶è¯­å¥
            r'for\s*\(',            # å¾ªç¯è¯­å¥
            r'while\s*\(',          # whileå¾ªç¯
        ]
        
        is_code = any(re.search(pattern, text, re.MULTILINE) for pattern in code_patterns)
        
        if is_code:
            # ä»£ç æ–‡æœ¬é€šå¸¸tokenå¯†åº¦æ›´é«˜ï¼ˆæ›´å¤šç¬¦å·å’Œå…³é”®å­—ï¼‰
            return base_ratio * 0.75
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«å¤§é‡ä¸­æ–‡
        chinese_char_count = len(re.findall(r'[\u4e00-\u9fff]', text))
        chinese_ratio = chinese_char_count / len(text) if text else 0
        
        if chinese_ratio > 0.3:  # è¶…è¿‡30%ä¸­æ–‡å­—ç¬¦
            # ä¸­æ–‡å­—ç¬¦é€šå¸¸éœ€è¦æ›´å¤štoken
            return base_ratio * 1.2
        
        return base_ratio
    
    def estimate_cost(self, input_tokens: int, output_tokens: int = 1000) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        costs = MODEL_COSTS.get(self.model, {"input": 0.01, "output": 0.03})
        return (input_tokens * costs["input"] + output_tokens * costs["output"]) / 1000

class SimpleOpenAIClient:
    """ç®€åŒ–çš„OpenAIå®¢æˆ·ç«¯ - æœ€å°åŒ–åˆå§‹åŒ–å‚æ•°é¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜"""
    
    def __init__(self, api_key: str, base_url: Optional[str] = None):
        if not api_key:
            raise ValueError("API key is required")
            
        try:
            # ä½¿ç”¨æœ€åŸºæœ¬çš„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
            # æ˜ç¡®åªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼Œé¿å…ä¼ é€’ä¸å…¼å®¹çš„å‚æ•°å¦‚proxies
            client_kwargs = {
                "api_key": api_key,
                "timeout": 60.0,  # æ˜ç¡®è®¾ç½®è¶…æ—¶
            }
            
            if base_url:
                client_kwargs["base_url"] = base_url
                logger.info(f"OpenAI client initializing with custom base URL: {base_url}")
            
            self.client = openai.AsyncOpenAI(**client_kwargs)
            logger.info("OpenAI client initialized successfully")
                
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            # å¦‚æœbase_urlå¯¼è‡´é—®é¢˜ï¼Œå°è¯•ä»…ä½¿ç”¨API key
            if base_url:
                try:
                    logger.warning("Retrying without custom base URL")
                    self.client = openai.AsyncOpenAI(
                        api_key=api_key,
                        timeout=60.0
                    )
                    logger.info("OpenAI client initialized with default settings as fallback")
                except Exception as e2:
                    raise RuntimeError(f"Cannot initialize OpenAI client: {e2}")
            else:
                raise RuntimeError(f"Cannot initialize OpenAI client: {e}")
    
    async def chat_completion(self, messages: List[Dict], model: str, 
                            response_format: Optional[Dict] = None, **kwargs) -> str:
        """å‘é€èŠå¤©å®Œæˆè¯·æ±‚ - å¸¦è¯¦ç»†æ—¥å¿—è®°å½•"""
        import time
        
        # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è®¡ç®—è¾“å…¥tokenæ•°é‡
        input_text = "\n".join([msg.get('content', '') for msg in messages])
        input_tokens = len(input_text) // 4  # ç®€å•ä¼°ç®—
        
        # è®°å½•è¯·æ±‚å‚æ•°è¯¦æƒ…
        logger.info("=" * 60)
        logger.info("ğŸš€ OpenAI API è°ƒç”¨å¼€å§‹")
        logger.info("=" * 60)
        logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°:")
        logger.info(f"  - æ¨¡å‹: {model}")
        logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
        logger.info(f"  - ä¼°ç®—è¾“å…¥tokens: {input_tokens}")
        logger.info(f"  - é¢å¤–å‚æ•°: {kwargs}")
        
        # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆå¯é€‰æ‹©æ€§è®°å½•ï¼‰
        logger.info(f"ğŸ’¬ æ¶ˆæ¯å†…å®¹:")
        for i, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')[:200] + '...' if len(msg.get('content', '')) > 200 else msg.get('content', '')
            logger.info(f"  [{i+1}] {role}: {content}")
        
        try:
            # å‘é€APIè¯·æ±‚
            logger.info("â³ æ­£åœ¨è°ƒç”¨OpenAI API...")
            
            # æ„å»ºAPIè°ƒç”¨å‚æ•°
            api_params = {
                "model": model,
                "messages": messages,
                **kwargs
            }
            
            # å¦‚æœæ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ response_format
            if response_format and self._supports_structured_output(model):
                api_params["response_format"] = {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "code_review_response",
                        "schema": response_format
                    }
                }
                logger.info("ğŸ¯ ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼")
            elif response_format:
                logger.info("ğŸ“ æ¨¡å‹ä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œä½¿ç”¨æç¤ºè¯çº¦æŸ")
            
            response = await self.client.chat.completions.create(**api_params)
            
            # è®¡ç®—å“åº”æ—¶é—´
            end_time = time.time()
            response_time = end_time - start_time
            
            # è·å–å“åº”å†…å®¹
            response_content = response.choices[0].message.content
            response_tokens = len(response_content) // 4  # ç®€å•ä¼°ç®—
            
            # è®°å½•å“åº”è¯¦æƒ…
            logger.info("=" * 60)
            logger.info("âœ… OpenAI API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info(f"â±ï¸  å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            logger.info(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
            
            # å°è¯•è·å–å®é™…tokenä½¿ç”¨é‡ï¼ˆå¦‚æœAPIè¿”å›äº†ï¼‰
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"  - è¾“å…¥tokens: {response.usage.prompt_tokens}")
                logger.info(f"  - è¾“å‡ºtokens: {response.usage.completion_tokens}")
                logger.info(f"  - æ€»tokens: {response.usage.total_tokens}")
            else:
                logger.info(f"  - ä¼°ç®—è¾“å…¥tokens: {input_tokens}")
                logger.info(f"  - ä¼°ç®—è¾“å‡ºtokens: {response_tokens}")
                
            logger.info(f"ğŸ¯ å“åº”å†…å®¹:")
            # è®°å½•å®Œæ•´å“åº”å†…å®¹ï¼Œä½†å¦‚æœå¤ªé•¿åˆ™æˆªå–
            if len(response_content) > 2000:
                logger.info(f"  {response_content[:800]}...")
                logger.info(f"  ... [ä¸­é—´çœç•¥ {len(response_content)-1600} ä¸ªå­—ç¬¦] ...")
                logger.info(f"  ...{response_content[-800:]}")
            else:
                logger.info(f"  {response_content}")
            
            # ä¸ºè°ƒè¯•ç›®çš„ï¼Œè®°å½•å“åº”çš„å‰1000ä¸ªå­—ç¬¦åˆ°DEBUGçº§åˆ«
            logger.debug(f"Full response preview: {response_content[:1000]}")
            
            logger.info("=" * 60)
            
            return response_content
            
        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            
            logger.error("=" * 60)
            logger.error("âŒ OpenAI API è°ƒç”¨å¤±è´¥")
            logger.error("=" * 60)
            logger.error(f"â±ï¸  å¤±è´¥æ—¶é—´: {response_time:.2f}ç§’")
            logger.error(f"ğŸ’¥ é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error("=" * 60)
            raise
    
    async def close(self):
        """å®‰å…¨å…³é—­å®¢æˆ·ç«¯ï¼Œé¿å…çŠ¶æ€é”™è¯¯"""
        try:
            if hasattr(self, 'client') and self.client:
                # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æœ‰closeæ–¹æ³•ä¸”å¯ä»¥å®‰å…¨è°ƒç”¨
                if hasattr(self.client, 'close'):
                    await self.client.close()
                    logger.info("OpenAI client closed successfully")
        except Exception as e:
            # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯ï¼Œé¿å…å½±å“ä¸»è¦åŠŸèƒ½
            logger.warning(f"Error closing OpenAI client (ignored): {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            if hasattr(self, 'client') and self.client:
                # åœ¨åŒæ­¥ç¯å¢ƒä¸­æ— æ³•è°ƒç”¨å¼‚æ­¥closeï¼Œåªè®°å½•æ—¥å¿—
                logger.debug("OpenAI client cleanup in destructor")
        except:
            pass  # å¿½ç•¥ææ„å‡½æ•°ä¸­çš„æ‰€æœ‰é”™è¯¯
    
    def _supports_structured_output(self, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒç»“æ„åŒ–è¾“å‡º"""
        # é¦–å…ˆæ£€æŸ¥å…¨å±€é…ç½®
        if not settings.enable_structured_output:
            return False
        
        # å¦‚æœå¼ºåˆ¶å¯ç”¨ï¼Œç›´æ¥è¿”å›True
        if settings.force_structured_output:
            return True
        
        # OpenAI GPT-4 å’Œ GPT-3.5-turbo çš„è¾ƒæ–°ç‰ˆæœ¬æ”¯æŒç»“æ„åŒ–è¾“å‡º
        # DeepSeek å’Œå…¶ä»–ç°ä»£æ¨¡å‹ä¹Ÿæ”¯æŒç»“æ„åŒ–è¾“å‡º
        supported_models = [
            "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", 
            "gpt-4", "gpt-3.5-turbo",
            "deepseek", "claude", "gemini"  # æ·»åŠ æ›´å¤šæ¨¡å‹æ”¯æŒ
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ¨¡å‹
        for supported in supported_models:
            if model.startswith(supported):
                return True
        
        # å¯¹äºè‡ªå®šä¹‰æ¨¡å‹ï¼Œæ ¹æ®é…ç½®å†³å®š
        return settings.force_structured_output

class SimpleAIProcessor:
    """ç®€åŒ–AIä»£ç åˆ†æå¤„ç†å™¨"""
    
    def __init__(self, model: str = None):
        self.model = model or settings.default_ai_model
        self.fallback_model = settings.fallback_ai_model
        self.token_manager = TokenManager(self.model)
        
        # å»¶è¿Ÿåˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œé¿å…å¯åŠ¨æ—¶çš„ä¾èµ–é—®é¢˜
        self._client = None
        
        logger.info(f"SimpleAIProcessor initialized with model: {self.model}")
    
    @property
    def client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œå¤±è´¥æ—¶è¿”å›Noneä»¥å¯ç”¨åŸºç¡€æ¨¡å¼"""
        if self._client is None:
            try:
                self._client = SimpleOpenAIClient(
                    api_key=settings.openai_api_key,
                    base_url=settings.api_base_url
                )
                logger.info("OpenAI client initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                logger.warning("AI client unavailable, will use basic analysis mode only")
                # è¿”å›Falseè¡¨ç¤ºå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                return False
        return self._client
    
    def _is_ai_available(self) -> bool:
        """æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not False
    
    async def analyze_merge_request(self, diff_files: List[FilePatchInfo],
                                  review_type: str, mr_info: Dict,
                                  historical_issues: Optional[Dict[str, List[Dict]]] = None) -> Dict[str, Any]:
        """åˆ†æMRçš„ä¸»å…¥å£å‡½æ•°"""
        import time

        # è®°å½•åˆ†æå¼€å§‹
        start_time = time.time()
        logger.info("ğŸ”" + "=" * 80)
        logger.info("ğŸ” å¼€å§‹ä»£ç å®¡æŸ¥åˆ†æ")
        logger.info("ğŸ”" + "=" * 80)
        logger.info(f"ğŸ“‹ å®¡æŸ¥å‚æ•°:")
        logger.info(f"  - å®¡æŸ¥ç±»å‹: {review_type}")
        logger.info(f"  - æ–‡ä»¶æ•°é‡: {len(diff_files)}")
        logger.info(f"  - AIæ¨¡å‹: {self.model}")
        logger.info(f"  - AIå®¢æˆ·ç«¯å¯ç”¨: {self._is_ai_available()}")
        logger.info(f"  - é€æ–‡ä»¶å®¡æŸ¥: {settings.enable_per_file_review}")
        logger.info(f"  - å†å²é—®é¢˜: {len(historical_issues or {})} ä¸ªæ–‡ä»¶æœ‰å†å²è®°å½•")

        logger.info(f"ğŸ“ å˜æ›´æ–‡ä»¶åˆ—è¡¨:")
        for i, file_patch in enumerate(diff_files):
            has_history = file_patch.filename in (historical_issues or {})
            history_mark = "ğŸ“Œ (æœ‰å†å²é—®é¢˜)" if has_history else ""
            logger.info(f"  [{i+1}] {file_patch.filename} ({file_patch.edit_type}) {history_mark}")

        logger.info(f"ğŸ¯ MRä¿¡æ¯:")
        logger.info(f"  - æ ‡é¢˜: {mr_info.get('title', 'æœªçŸ¥')}")
        logger.info(f"  - æºåˆ†æ”¯: {mr_info.get('source_branch', 'æœªçŸ¥')}")
        logger.info(f"  - ç›®æ ‡åˆ†æ”¯: {mr_info.get('target_branch', 'æœªçŸ¥')}")

        try:
            logger.info(f"âš¡ å¼€å§‹æ‰§è¡Œ {review_type} ç±»å‹å®¡æŸ¥...")

            # æ ¹æ®å®¡æŸ¥ç±»å‹å’Œé…ç½®é€‰æ‹©åˆ†æç­–ç•¥
            if settings.enable_per_file_review and len(diff_files) > 1:
                result = await self._per_file_analysis(diff_files, review_type, mr_info, historical_issues)
            elif review_type == "security":
                result = await self._security_focused_analysis(diff_files, mr_info)
            elif review_type == "performance":
                result = await self._performance_focused_analysis(diff_files, mr_info)
            elif review_type == "quick":
                result = await self._quick_analysis(diff_files, mr_info)
            else:  # full analysis
                result = await self._comprehensive_analysis(diff_files, mr_info)

            # ä¼°ç®—æˆæœ¬
            cost_estimate = self._estimate_analysis_cost(diff_files)
            result["cost_estimate"] = cost_estimate

            # è®°å½•åˆ†æå®Œæˆ
            end_time = time.time()
            analysis_time = end_time - start_time

            logger.info("âœ…" + "=" * 80)
            logger.info("âœ… ä»£ç å®¡æŸ¥åˆ†æå®Œæˆ")
            logger.info("âœ…" + "=" * 80)
            logger.info(f"â±ï¸  æ€»åˆ†ææ—¶é—´: {analysis_time:.2f}ç§’")
            logger.info(f"ğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ:")
            logger.info(f"  - å®¡æŸ¥ç±»å‹: {result.get('type', 'unknown')}")
            logger.info(f"  - è¯„åˆ†: {result.get('score', 0):.1f}/10.0")
            logger.info(f"  - å‘ç°é—®é¢˜æ•°: {len(result.get('findings', []))}")
            logger.info(f"  - å»ºè®®æ•°: {len(result.get('suggestions', []))}")
            logger.info(f"  - æ¨èæ•°: {len(result.get('recommendations', []))}")
            logger.info(f"  - æˆæœ¬ä¼°ç®—: ${cost_estimate:.4f}")

            if result.get('findings'):
                logger.info(f"ğŸ” å‘ç°çš„ä¸»è¦é—®é¢˜:")
                for i, finding in enumerate(result['findings'][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"  [{i+1}] {finding.get('filename', 'N/A')}: {finding.get('message', finding.get('description', 'N/A'))}")
                if len(result['findings']) > 3:
                    logger.info(f"  ... è¿˜æœ‰ {len(result['findings']) - 3} ä¸ªé—®é¢˜")

            logger.info(f"ğŸ“ æ€»ç»“: {result.get('summary', 'æ— æ€»ç»“')}")
            logger.info("âœ…" + "=" * 80)

            logger.info(f"Analysis completed with score: {result['score']}")
            return result

        except Exception as e:
            end_time = time.time()
            analysis_time = end_time - start_time

            logger.error("âŒ" + "=" * 80)
            logger.error("âŒ ä»£ç å®¡æŸ¥åˆ†æå¤±è´¥")
            logger.error("âŒ" + "=" * 80)
            logger.error(f"â±ï¸  å¤±è´¥æ—¶é—´: {analysis_time:.2f}ç§’")
            logger.error(f"ğŸ’¥ é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error("âŒ" + "=" * 80)
            logger.error(f"AI analysis failed: {e}")
            raise
        finally:
            # ç¡®ä¿å®¢æˆ·ç«¯æ­£ç¡®æ¸…ç†
            await self._cleanup_client()
    
    async def _cleanup_client(self):
        """æ¸…ç†AIå®¢æˆ·ç«¯èµ„æº"""
        try:
            if self._client and self._client is not False:
                await self._client.close()
                logger.debug("AI client resources cleaned up")
        except Exception as e:
            # å¿½ç•¥æ¸…ç†é”™è¯¯ï¼Œé¿å…å½±å“ä¸»è¦æµç¨‹
            logger.debug(f"Client cleanup error (ignored): {e}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._cleanup_client()
    
    async def _per_file_analysis(self, diff_files: List[FilePatchInfo],
                               review_type: str, mr_info: Dict,
                               historical_issues: Optional[Dict[str, List[Dict]]] = None) -> Dict[str, Any]:
        """é€æ–‡ä»¶å¹¶è¡Œåˆ†æ - æ–°çš„æ ¸å¿ƒæ–¹æ³•"""
        logger.info(f"ğŸš€ å¯åŠ¨é€æ–‡ä»¶å¹¶è¡Œåˆ†æï¼Œæ–‡ä»¶æ•°é‡: {len(diff_files)}")
        logger.info(f"ğŸ”§ æœ€å¤§å¹¶å‘æ•°: {settings.max_concurrent_file_reviews}")

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(settings.max_concurrent_file_reviews)

        async def analyze_single_file(file_patch: FilePatchInfo) -> Dict[str, Any]:
            async with semaphore:
                # è·å–è¯¥æ–‡ä»¶çš„å†å²é—®é¢˜
                file_historical_issues = (historical_issues or {}).get(file_patch.filename, [])
                return await self._analyze_single_file(file_patch, review_type, file_historical_issues)

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
        start_time = time.time()
        tasks = [analyze_single_file(file_patch) for file_patch in diff_files]
        file_results = await asyncio.gather(*tasks, return_exceptions=True)

        parallel_time = time.time() - start_time
        logger.info(f"âš¡ å¹¶è¡Œæ–‡ä»¶åˆ†æå®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}ç§’")

        # èšåˆç»“æœ
        all_findings = []
        all_suggestions = []
        failed_files = []

        for i, result in enumerate(file_results):
            if isinstance(result, Exception):
                logger.error(f"æ–‡ä»¶ {diff_files[i].filename} åˆ†æå¤±è´¥: {result}")
                failed_files.append(diff_files[i].filename)
                continue

            all_findings.extend(result.get("findings", []))
            all_suggestions.extend(result.get("suggestions", []))

        # ç”Ÿæˆå…¨å±€æ€»ç»“
        global_summary = await self._generate_global_summary(
            all_findings, all_suggestions, mr_info, failed_files
        )

        # è®¡ç®—æ•´ä½“è¯„åˆ†
        score = self._calculate_overall_score(all_findings, diff_files, failed_files)

        logger.info(f"ğŸ“Š é€æ–‡ä»¶åˆ†æå®Œæˆ:")
        logger.info(f"  - æˆåŠŸåˆ†ææ–‡ä»¶: {len(diff_files) - len(failed_files)}")
        logger.info(f"  - å¤±è´¥æ–‡ä»¶: {len(failed_files)}")
        logger.info(f"  - æ€»é—®é¢˜æ•°: {len(all_findings)}")
        logger.info(f"  - æ€»å»ºè®®æ•°: {len(all_suggestions)}")

        return {
            "type": f"{review_type}_per_file",
            "findings": all_findings[:30],  # é™åˆ¶æ•°é‡é˜²æ­¢ç»“æœè¿‡å¤§
            "suggestions": all_suggestions[:20],
            "recommendations": self._generate_recommendations(all_findings),
            "score": score,
            "summary": global_summary,
            "failed_files": failed_files,
            "parallel_analysis_time": parallel_time
        }
    
    async def _analyze_single_file(self, file_patch: FilePatchInfo,
                                 review_type: str,
                                 historical_issues: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        import time

        start_time = time.time()
        logger.info(f"ğŸ“„ å¼€å§‹åˆ†ææ–‡ä»¶: {file_patch.filename}")

        if historical_issues:
            logger.info(f"  ğŸ“Œ å‘ç° {len(historical_issues)} ä¸ªå†å²é—®é¢˜éœ€è¦å…³æ³¨")

        try:
            # åŸºç¡€é—®é¢˜æ£€æµ‹ï¼ˆæœ¬åœ°ï¼Œå¿«é€Ÿï¼‰
            # basic_issues = self._detect_basic_issues(file_patch)

            # æ„å»ºå®Œæ•´æ–‡ä»¶å†…å®¹ç”¨äºAIåˆ†æ
            full_file_content = self._prepare_file_content_for_analysis(file_patch)

            # AIæ·±åº¦åˆ†æ
            ai_findings = []
            ai_suggestions = []

            if self._is_ai_available() and full_file_content:
                try:
                    ai_result = await self._ai_single_file_analysis(
                        file_patch, full_file_content, review_type, historical_issues
                    )
                    ai_findings = ai_result.get("findings", [])
                    ai_suggestions = ai_result.get("suggestions", [])
                except Exception as e:
                    logger.warning(f"æ–‡ä»¶ {file_patch.filename} çš„AIåˆ†æå¤±è´¥: {e}")

            # åˆå¹¶ç»“æœ
            #all_findings = basic_issues + ai_findings
            all_findings = ai_findings

            end_time = time.time()
            analysis_time = end_time - start_time

            logger.info(f"âœ… æ–‡ä»¶ {file_patch.filename} åˆ†æå®Œæˆ ({analysis_time:.2f}ç§’)")
            logger.info(f"    - åŸºç¡€é—®é¢˜: {len(basic_issues)}ä¸ª")
            logger.info(f"    - AIå‘ç°é—®é¢˜: {len(ai_findings)}ä¸ª")
            logger.info(f"    - AIå»ºè®®: {len(ai_suggestions)}ä¸ª")

            return {
                "filename": file_patch.filename,
                "findings": all_findings,
                "suggestions": ai_suggestions,
                "analysis_time": analysis_time
            }

        except Exception as e:
            end_time = time.time()
            logger.error(f"âŒ æ–‡ä»¶ {file_patch.filename} åˆ†æå¤±è´¥: {e} ({end_time - start_time:.2f}ç§’)")
            raise
    
    def _prepare_file_content_for_analysis(self, file_patch: FilePatchInfo) -> str:
        """å‡†å¤‡ç”¨äºAIåˆ†æçš„æ–‡ä»¶å†…å®¹ï¼ˆåŒ…å«å®Œæ•´å†…å®¹å’Œdiffï¼‰"""
        logger.debug(f"å‡†å¤‡æ–‡ä»¶å†…å®¹: {file_patch.filename}")
        
        # è·å–æ–‡ä»¶å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨new_contentï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨old_contentï¼‰
        full_content = file_patch.new_content or file_patch.old_content
        
        if not full_content:
            logger.debug(f"æ–‡ä»¶ {file_patch.filename} æ— å®Œæ•´å†…å®¹ï¼Œä»…ä½¿ç”¨diff")
            return file_patch.patch
        
        # æŒ‰è¡Œæˆªæ–­æ–‡ä»¶å†…å®¹
        lines = full_content.splitlines()
        if len(lines) > settings.max_file_lines:
            logger.info(f"æ–‡ä»¶ {file_patch.filename} è¡Œæ•°è¿‡å¤š({len(lines)})ï¼Œæˆªæ–­è‡³{settings.max_file_lines}è¡Œ")
            truncated_content = '\n'.join(lines[:settings.max_file_lines])
            truncated_content += f"\n... [æ–‡ä»¶è¢«æˆªæ–­ï¼ŒåŸå§‹è¡Œæ•°: {len(lines)}]"
        else:
            truncated_content = full_content
        
        # ç»„åˆå®Œæ•´å†…å®¹å’Œå˜æ›´ä¿¡æ¯
        content_for_analysis = f"""
æ–‡ä»¶: {file_patch.filename}
å˜æ›´ç±»å‹: {file_patch.edit_type}

å®Œæ•´æ–‡ä»¶å†…å®¹:
```
{truncated_content}
```

å˜æ›´è¯¦æƒ…(diff):
```diff
{file_patch.patch}
```
"""
        return content_for_analysis.strip()
    
    async def _ai_single_file_analysis(self, file_patch: FilePatchInfo,
                                     full_content: str, review_type: str,
                                     historical_issues: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡ŒAIåˆ†æ"""
        if not self._is_ai_available():
            return {"findings": [], "suggestions": []}

        # æ ¹æ®å®¡æŸ¥ç±»å‹æ„å»ºä¸åŒçš„æç¤ºè¯
        focus_areas = REVIEW_TYPES.get(review_type, {}).get("focus_areas", ["quality"])
        focus_description = ", ".join(focus_areas)

        # æ„å»ºå†å²é—®é¢˜çš„æç¤º - å¼ºåˆ¶å…³æ³¨å†å²é—®é¢˜
        historical_context = ""

        if historical_issues:
            historical_context = "\n\nğŸ”´ **é‡è¦ï¼šæœ¬æ¬¡å®¡æŸ¥ä¸ºå†å²é—®é¢˜è¿½è¸ªæ¨¡å¼**\n\n"
            historical_context += "âš ï¸ **ä¸Šä¸€æ¬¡å®¡æŸ¥å‘ç°çš„é—®é¢˜æ¸…å•**ï¼š\n\n"

            for i, issue in enumerate(historical_issues, 1):
                historical_context += f"{i}. [{issue.get('severity', 'low').upper()}] {issue.get('type', 'unknown')}\n"
                historical_context += f"   æè¿°: {issue.get('description', 'N/A')}\n"
                if issue.get('line_number'):
                    historical_context += f"   ä½ç½®: ç¬¬{issue.get('line_number')}è¡Œé™„è¿‘\n"
                historical_context += f"   ä¿®å¤å»ºè®®: {issue.get('suggestion', 'N/A')}\n\n"

            historical_context += "\nğŸ¯ **å¼ºåˆ¶å®¡æŸ¥è¦æ±‚ï¼ˆæå…¶é‡è¦ï¼‰**ï¼š\n"
            historical_context += "1. **ä»…å…³æ³¨å†å²é—®é¢˜**ï¼šåªæ£€æŸ¥ä¸Šè¿°å†å²é—®é¢˜ç›¸å…³çš„ä»£ç åŒºåŸŸæ˜¯å¦æœ‰ä¿®æ”¹\n"
            historical_context += "2. **æ£€æŸ¥ä¿®å¤çŠ¶æ€**ï¼šå¯¹æ¯ä¸ªå†å²é—®é¢˜ï¼Œåˆ¤æ–­æœ¬æ¬¡ diff ä¸­æ˜¯å¦å·²ä¿®å¤\n"
            historical_context += "3. **ä¿®å¤å³æ¸…é™¤**ï¼šå¦‚æœå†å²é—®é¢˜å·²è¢«æ­£ç¡®ä¿®å¤ä¸”æ²¡æœ‰å¼•å…¥æ–°çš„ä¸¥é‡é—®é¢˜ï¼Œè¿”å›ç©º findings []\n"
            historical_context += "4. **é—®é¢˜æŒç»­**ï¼šå¦‚æœå†å²é—®é¢˜ä»ç„¶å­˜åœ¨ï¼ˆä»£ç æœªæ”¹æˆ–æ”¹å¾—ä¸å¯¹ï¼‰ï¼Œåœ¨ findings ä¸­æŠ¥å‘Š\n"
            historical_context += "5. **æ–°é—®é¢˜ä¸¥æ ¼é™åˆ¶**ï¼šé™¤éæ˜¯ä¸¥é‡å½±å“ç”Ÿäº§çš„æ–°é—®é¢˜ï¼ˆå¦‚å®‰å…¨æ¼æ´ã€å´©æºƒé£é™©ï¼‰ï¼Œå¦åˆ™ä¸è¦æŠ¥å‘Šæ–°é—®é¢˜\n"
            historical_context += "6. **ä¸è¦æŠ¥å‘Šå…¶ä»–ä»£ç **ï¼šä¸è¦å®¡æŸ¥ä¸å†å²é—®é¢˜æ— å…³çš„ä»£ç åŒºåŸŸ\n\n"
            historical_context += "ğŸ’¡ **ç†æƒ³ç»“æœ**ï¼šå¦‚æœå¼€å‘è€…æ­£ç¡®ä¿®å¤äº†æ‰€æœ‰å†å²é—®é¢˜ï¼Œä½ åº”è¯¥è¿”å› findings: []\n\n"

        base_prompt = f"""
è¯·ä¸“é—¨åˆ†æä»¥ä¸‹æ–‡ä»¶çš„ä»£ç å˜æ›´ã€‚

{full_content}
{historical_context}

{'ğŸ”´ **å®¡æŸ¥æ¨¡å¼ï¼šå†å²é—®é¢˜è¿½è¸ªæ¨¡å¼**' if historical_issues else 'ğŸ“‹ **å®¡æŸ¥æ¨¡å¼ï¼šå¸¸è§„ä»£ç å®¡æŸ¥**'}

{'**å®¡æŸ¥è¦æ±‚ï¼ˆå†å²é—®é¢˜æ¨¡å¼ï¼‰**ï¼š' if historical_issues else '**åˆ†æè¦æ±‚**ï¼š'}
{'''
1. **åªçœ‹å†å²é—®é¢˜åŒºåŸŸ**ï¼šä»…æ£€æŸ¥ä¸Šè¿°å†å²é—®é¢˜æ‰€åœ¨çš„ä»£ç è¡ŒåŠå…¶å‘¨è¾¹ä»£ç åœ¨æœ¬æ¬¡ diff ä¸­çš„ä¿®æ”¹æƒ…å†µ
2. **åˆ¤æ–­ä¿®å¤çŠ¶æ€**ï¼š
   - å¦‚æœé—®é¢˜å·²ä¿®å¤ä¸”æ²¡å¼•å…¥æ–°çš„ä¸¥é‡bug â†’ findings è¿”å›ç©ºæ•°ç»„ []
   - å¦‚æœé—®é¢˜æœªä¿®å¤æˆ–ä¿®å¤ä¸å½“ â†’ findings ä¸­æŠ¥å‘Šè¯¥å†å²é—®é¢˜
   - å¦‚æœå¼•å…¥äº†ä¸¥é‡çš„æ–°é—®é¢˜ï¼ˆå¦‚å®‰å…¨æ¼æ´ï¼‰â†’ findings ä¸­æŠ¥å‘Šæ–°é—®é¢˜
3. **ä¸¥æ ¼é™åˆ¶æ–°é—®é¢˜**ï¼šä¸è¦å®¡æŸ¥ä¸å†å²é—®é¢˜æ— å…³çš„ä»£ç ï¼Œä¸è¦æŠ¥å‘Šå°é—®é¢˜æˆ–å»ºè®®æ€§é—®é¢˜
4. **ç›®æ ‡**ï¼šå¸®åŠ©å¼€å‘è€…ç¡®è®¤å†å²é—®é¢˜æ˜¯å¦å·²ä¿®å¤ï¼Œè€Œä¸æ˜¯å‘ç°æ–°é—®é¢˜
''' if historical_issues else '''
1. **é¿å…é‡å¤å‘ç°**ï¼šå¯¹äºåŒä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼Œåˆå¹¶åˆ°ä¸€ä¸ª finding
2. **åˆå¹¶ç›¸å…³é—®é¢˜**ï¼šåŒä¸€ä»£ç è¡Œçš„å¤šä¸ªé—®é¢˜æ•´åˆä¸ºä¸€ä¸ªæè¿°
3. **èšç„¦å…³é”®é—®é¢˜**ï¼šåªæŠ¥å‘Šå¯¹ä»£ç åŠŸèƒ½å’Œè´¨é‡æœ‰é‡å¤§å½±å“çš„é—®é¢˜ï¼ˆ0-3ä¸ªï¼‰
4. **åªå®¡æŸ¥ diff**ï¼šåªå…³æ³¨ diff éƒ¨åˆ†çš„å˜æ›´ï¼Œä¸è¦å®¡æŸ¥æ•´ä¸ªæ–‡ä»¶å†…å®¹
5. **æ— ä¼¤å¤§é›…çš„é—®é¢˜ä¸æŠ¥å‘Š**ï¼šä»£ç é£æ ¼ã€æ³¨é‡Šå»ºè®®ç­‰å°é—®é¢˜ä¸è¦æŠ¥å‘Š
'''}

æ³¨æ„ï¼šè¿™æ˜¯å•æ–‡ä»¶åˆ†æã€‚{' âš ï¸ æœ¬æ¬¡æ˜¯å†å²é—®é¢˜è¿½è¸ªï¼Œå¦‚æœå†å²é—®é¢˜éƒ½å·²ä¿®å¤ï¼Œè¯·è¿”å›ç©ºçš„ findings: []' if historical_issues else ''}
"""

        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """

âš ï¸ é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ã€è§£é‡Šæˆ–markdownæ ‡è®°ï¼š

{
    "findings": [
        {
            "type": "é—®é¢˜ç±»å‹",
            "line_number": è¡Œå·,
            "severity": "high/medium/low",
            "description": "å®Œæ•´çš„é—®é¢˜æè¿°ï¼ˆå¦‚æœåŒä¸€è¡Œæœ‰å¤šä¸ªç›¸å…³é—®é¢˜ï¼Œè¯·åˆå¹¶æè¿°ï¼‰",
            "suggestion": "ç»¼åˆçš„ä¿®å¤å»ºè®®"
        }
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}

æ ¼å¼è¦æ±‚ï¼š
1. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
2. ä¸è¦ç”¨```jsonæˆ–å…¶ä»–ä»£ç å—åŒ…è£…
3. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—
4. ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²éƒ½ç”¨åŒå¼•å·åŒ…å›´
5. ç¡®ä¿åŒä¸€è¡Œæˆ–ç›¸å…³çš„é—®é¢˜åªç”Ÿæˆä¸€ä¸ªfindingå¯¹è±¡
"""
        else:
            prompt = base_prompt

        try:
            response = await self.client.chat_completion(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œä¸“æ³¨äºå•æ–‡ä»¶ä»£ç åˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                model=self.model,
                temperature=0.2,
                max_tokens=2000,
                response_format={
                    "type": "object",
                    "properties": {
                        "findings": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "type": {"type": "string"},
                                    "line_number": {"type": "integer"},
                                    "severity": {"type": "string", "enum": ["high", "medium", "low"]},
                                    "description": {"type": "string"},
                                    "suggestion": {"type": "string"}
                                },
                                "required": ["type", "severity", "description"]
                            }
                        },
                        "suggestions": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    },
                    "required": ["findings", "suggestions"]
                }
            )

            # è§£æå“åº”
            cleaned_response = self._extract_json_from_response(response)

            try:
                result = json.loads(cleaned_response)
                logger.debug("Successfully parsed JSON response")
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                logger.error(f"å°è¯•è§£æçš„JSON: {cleaned_response[:500]}...")

                # å°è¯•ä½¿ç”¨æ›´æ¿€è¿›çš„ä¿®å¤æ–¹æ³•
                try:
                    fixed_json = self._aggressive_json_fix(cleaned_response)
                    result = json.loads(fixed_json)
                    logger.info("ä½¿ç”¨æ¿€è¿›ä¿®å¤æ–¹æ³•æˆåŠŸè§£æJSON")
                except Exception as e2:
                    logger.error(f"æ¿€è¿›ä¿®å¤ä¹Ÿå¤±è´¥: {e2}")
                    # è¿”å›é»˜è®¤ç»“æ„
                    return {
                        "findings": [],
                        "suggestions": [f"AIåˆ†æå¤±è´¥ï¼ŒJSONè§£æé”™è¯¯: {str(e)[:100]}"]
                    }

            # éªŒè¯å¹¶ä¿®å¤ç»“æœç»“æ„
            result = self._validate_and_fix_result(result)

            # ä¸ºæ¯ä¸ªfindingæ·»åŠ filename
            for finding in result.get("findings", []):
                finding["filename"] = file_patch.filename

            return result

        except Exception as e:
            logger.error(f"AIå•æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return {"findings": [], "suggestions": []}
    
    async def _generate_global_summary(self, all_findings: List[Dict], 
                                     all_suggestions: List[str], mr_info: Dict,
                                     failed_files: List[str]) -> str:
        """ç”Ÿæˆå…¨å±€åˆ†ææ€»ç»“"""
        if not self._is_ai_available():
            high_issues = len([f for f in all_findings if f.get("severity") == "high"])
            medium_issues = len([f for f in all_findings if f.get("severity") == "medium"])
            return f"é€æ–‡ä»¶åˆ†æå®Œæˆï¼Œå‘ç°{high_issues}ä¸ªé«˜é£é™©é—®é¢˜ï¼Œ{medium_issues}ä¸ªä¸­ç­‰é£é™©é—®é¢˜ã€‚"
        
        # ç»Ÿè®¡ä¿¡æ¯
        high_issues = len([f for f in all_findings if f.get("severity") == "high"])
        medium_issues = len([f for f in all_findings if f.get("severity") == "medium"])
        low_issues = len([f for f in all_findings if f.get("severity") == "low"])
        
        # é—®é¢˜åˆ†ç±»ç»Ÿè®¡
        issue_types = {}
        for finding in all_findings:
            issue_type = finding.get("type", "unknown")
            issue_types[issue_type] = issue_types.get(issue_type, 0) + 1
        
        top_issues = sorted(issue_types.items(), key=lambda x: x[1], reverse=True)[:5]
        
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹é€æ–‡ä»¶ä»£ç å®¡æŸ¥ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…¨å±€æ€»ç»“ï¼š

MRä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- æºåˆ†æ”¯ï¼š{mr_info.get('source_branch', 'æœªçŸ¥')}
- ç›®æ ‡åˆ†æ”¯ï¼š{mr_info.get('target_branch', 'æœªçŸ¥')}

åˆ†æç»Ÿè®¡ï¼š
- é«˜é£é™©é—®é¢˜ï¼š{high_issues}ä¸ª
- ä¸­ç­‰é£é™©é—®é¢˜ï¼š{medium_issues}ä¸ª
- ä½é£é™©é—®é¢˜ï¼š{low_issues}ä¸ª
- åˆ†æå¤±è´¥æ–‡ä»¶ï¼š{len(failed_files)}ä¸ª

ä¸»è¦é—®é¢˜ç±»å‹ï¼š
{chr(10).join([f"- {issue_type}: {count}ä¸ª" for issue_type, count in top_issues])}

æ”¹è¿›å»ºè®®æ•°é‡ï¼š{len(all_suggestions)}ä¸ª

è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œè¯„ä¼°æ•´ä½“ä»£ç è´¨é‡å’Œä¸»è¦æ”¹è¿›æ–¹å‘ã€‚
"""
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.3,
                max_tokens=300
            )
            return response.strip()
        except Exception as e:
            logger.error(f"ç”Ÿæˆå…¨å±€æ€»ç»“å¤±è´¥: {e}")
            return f"é€æ–‡ä»¶å¹¶è¡Œåˆ†æå®Œæˆï¼Œæ€»å…±å‘ç°{len(all_findings)}ä¸ªé—®é¢˜ï¼Œ{len(all_suggestions)}ä¸ªå»ºè®®ã€‚"
    
    def _calculate_overall_score(self, all_findings: List[Dict], 
                               diff_files: List[FilePatchInfo], 
                               failed_files: List[str]) -> float:
        """è®¡ç®—æ•´ä½“è¯„åˆ†"""
        base_score = 8.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for finding in all_findings:
            severity = finding.get("severity", "low")
            if severity == "high":
                base_score -= 1.0
            elif severity == "medium":
                base_score -= 0.5
            else:
                base_score -= 0.2
        
        # æ ¹æ®å¤±è´¥æ–‡ä»¶æ‰£åˆ†
        if failed_files:
            failure_penalty = len(failed_files) * 0.5
            base_score -= failure_penalty
            logger.info(f"ç”±äº{len(failed_files)}ä¸ªæ–‡ä»¶åˆ†æå¤±è´¥ï¼Œæ‰£åˆ†{failure_penalty}")
        
        # æ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´
        if len(diff_files) > 10:
            base_score -= 0.3
        
        return max(base_score, 2.0)  # æœ€ä½2åˆ†
    
    async def _comprehensive_analysis(self, diff_files: List[FilePatchInfo], 
                                    mr_info: Dict) -> Dict[str, Any]:
        """å…¨é¢åˆ†æ"""
        all_findings = []
        all_suggestions = []
        
        # åŸºç¡€é—®é¢˜æ£€æµ‹
        for file_patch in diff_files:
            basic_issues = self._detect_basic_issues(file_patch)
            all_findings.extend(basic_issues)
        
        # AIæ·±åº¦åˆ†æ
        try:
            ai_analysis = await self._ai_comprehensive_analysis(diff_files, mr_info)
            all_findings.extend(ai_analysis.get("findings", []))
            all_suggestions.extend(ai_analysis.get("suggestions", []))
        except Exception as e:
            logger.warning(f"AI analysis failed, using basic analysis only: {e}")
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        score = self._calculate_comprehensive_score(all_findings, diff_files)
        
        return {
            "type": "comprehensive",
            "findings": all_findings[:20],  # é™åˆ¶æ•°é‡
            "suggestions": all_suggestions[:10],
            "recommendations": self._generate_recommendations(all_findings),
            "score": score,
            "summary": self._generate_summary(all_findings, score, len(diff_files))
        }
    
    async def _security_focused_analysis(self, diff_files: List[FilePatchInfo], 
                                       mr_info: Dict) -> Dict[str, Any]:
        """å®‰å…¨ä¸“é¡¹åˆ†æ"""
        security_issues = []
        
        for file_patch in diff_files:
            # æ£€æµ‹å¸¸è§å®‰å…¨é—®é¢˜
            issues = self._detect_security_issues(file_patch)
            security_issues.extend(issues)
            
            # å¯¹æ¯ä¸ªæ½œåœ¨é—®é¢˜è¿›è¡ŒAIæ·±åº¦åˆ†æ
            for issue in issues:
                try:
                    # ä»æè¿°ä¸­æå–ä»£ç è¡Œï¼ˆå¦‚æœéœ€è¦ï¼‰
                    code_line = issue.get("description", "").split("ä»£ç ï¼š")[-1] if "ä»£ç ï¼š" in issue.get("description", "") else ""
                    ai_result = await self._ai_security_analysis(
                        file_patch.filename, 
                        issue["line_number"], 
                        code_line,
                        issue["category"]
                    )
                    if ai_result["is_vulnerability"]:
                        security_issues.append(ai_result)
                except Exception as e:
                    logger.warning(f"Security AI analysis failed: {e}")
        
        security_score = self._calculate_security_score(security_issues)
        
        return {
            "type": "security",
            "findings": security_issues,
            "score": security_score,
            "summary": f"å‘ç° {len(security_issues)} ä¸ªå®‰å…¨é—®é¢˜",
            "recommendations": self._generate_security_recommendations(security_issues)
        }
    
    async def _performance_focused_analysis(self, diff_files: List[FilePatchInfo], 
                                          mr_info: Dict) -> Dict[str, Any]:
        """æ€§èƒ½ä¸“é¡¹åˆ†æ"""
        performance_issues = []
        
        for file_patch in diff_files:
            # æ£€æŸ¥å¸¸è§æ€§èƒ½é—®é¢˜
            issues = self._detect_performance_issues(file_patch)
            performance_issues.extend(issues)
        
        # ä½¿ç”¨AIè¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ
        if performance_issues:
            try:
                ai_analysis = await self._ai_performance_analysis(performance_issues, diff_files)
                performance_issues.extend(ai_analysis)
            except Exception as e:
                logger.warning(f"Performance AI analysis failed: {e}")
        
        performance_score = self._calculate_performance_score(performance_issues)
        
        return {
            "type": "performance",
            "findings": performance_issues,
            "score": performance_score,
            "summary": f"å‘ç° {len(performance_issues)} ä¸ªæ€§èƒ½é—®é¢˜",
            "recommendations": self._generate_performance_recommendations(performance_issues)
        }
    
    async def _quick_analysis(self, diff_files: List[FilePatchInfo], 
                            mr_info: Dict) -> Dict[str, Any]:
        """å¿«é€ŸåŸºç¡€åˆ†æ"""
        basic_issues = []
        
        for file_patch in diff_files:
            issues = self._detect_basic_issues(file_patch)
            basic_issues.extend(issues)
        
        # å¿«é€ŸAIæ€»ç»“
        try:
            quick_summary = await self._ai_quick_summary(diff_files, mr_info)
        except Exception as e:
            logger.warning(f"Quick AI summary failed: {e}")
            quick_summary = "å¿«é€Ÿåˆ†æå®Œæˆï¼Œå‘ç°åŸºç¡€é—®é¢˜ã€‚"
        
        score = 8.0 - min(len(basic_issues) * 0.5, 3.0)  # åŸºç¡€è¯„åˆ†é€»è¾‘
        
        return {
            "type": "quick",
            "findings": basic_issues[:10],  # é™åˆ¶æ•°é‡
            "score": max(score, 5.0),
            "summary": quick_summary,
            "recommendations": ["å»ºè®®è¿›è¡Œå®Œæ•´å®¡æŸ¥ä»¥å‘ç°æ›´å¤šé—®é¢˜"]
        }
    
    def _detect_basic_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹åŸºç¡€ä»£ç é—®é¢˜"""
        logger.info(f"ğŸ” å¼€å§‹åŸºç¡€é—®é¢˜æ£€æµ‹: {file_patch.filename}")
        
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        logger.info(f"  - æ–‡ä»¶ç±»å‹: {file_patch.edit_type}")
        logger.info(f"  - æ–°å¢è¡Œæ•°: {len(new_lines)}")
        
        detected_issues_by_type = {}
        
        for line_num, line in enumerate(new_lines, 1):
            line_content = line[1:].strip()  # ç§»é™¤'+'ç¬¦å·
            
            # æ£€æŸ¥åŸºç¡€é—®é¢˜
            if len(line_content) > 120:
                issue = {
                    "type": "line_too_long",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "description": f"ä»£ç è¡Œè¿‡é•¿ï¼ˆ{len(line_content)}å­—ç¬¦ï¼‰ï¼Œå»ºè®®ä¿æŒåœ¨120å­—ç¬¦ä»¥å†…ä»¥æé«˜å¯è¯»æ€§",
                    "suggestion": "å°†é•¿è¡Œä»£ç æ‹†åˆ†ä¸ºå¤šè¡Œï¼Œæˆ–è€…é‡æ„å¤æ‚çš„è¡¨è¾¾å¼",
                    "severity": "low"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("line_too_long", 0)
                detected_issues_by_type["line_too_long"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: ä»£ç è¡Œè¿‡é•¿ ({len(line_content)} å­—ç¬¦)")
            
            if re.search(r"console\.log|print\(.*\)|System\.out\.println", line_content):
                issue = {
                    "type": "debug_statement",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "description": f"æ£€æµ‹åˆ°ç–‘ä¼¼è°ƒè¯•è¯­å¥ï¼š{line_content[:50]}{'...' if len(line_content) > 50 else ''}ã€‚è°ƒè¯•è¯­å¥ä¸åº”æäº¤åˆ°ç”Ÿäº§ä»£ç ä¸­",
                    "suggestion": "ç§»é™¤è°ƒè¯•è¯­å¥ï¼Œæˆ–ä½¿ç”¨æ—¥å¿—ç³»ç»Ÿæ›¿ä»£",
                    "severity": "medium"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("debug_statement", 0)
                detected_issues_by_type["debug_statement"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: æ£€æµ‹åˆ°è°ƒè¯•è¯­å¥: {line_content[:50]}...")
            
            if re.search(r"TODO|FIXME|HACK", line_content, re.IGNORECASE):
                issue = {
                    "type": "todo_comment",
                    "filename": file_patch.filename,
                    "line_number": line_num,
                    "description": f"åŒ…å«å¾…åŠäº‹é¡¹æˆ–ä¿®å¤æ ‡è®°ï¼š{line_content[:50]}{'...' if len(line_content) > 50 else ''}ã€‚å»ºè®®åœ¨å‘å¸ƒå‰å¤„ç†æ‰€æœ‰TODOå’ŒFIXME",
                    "suggestion": "å®ŒæˆTODOé¡¹ç›®æˆ–åˆ›å»ºå¯¹åº”çš„å·¥å•æ¥è·Ÿè¸ªï¼Œé¿å…åœ¨ç”Ÿäº§ä»£ç ä¸­ç•™ä¸‹æœªå®Œæˆçš„æ ‡è®°",
                    "severity": "low"
                }
                issues.append(issue)
                detected_issues_by_type.setdefault("todo_comment", 0)
                detected_issues_by_type["todo_comment"] += 1
                logger.info(f"  âš ï¸  è¡Œ{line_num}: å‘ç°TODO/FIXME: {line_content[:50]}...")
        
        # æ€»ç»“æ£€æµ‹ç»“æœ
        if issues:
            logger.info(f"  ğŸ“Š {file_patch.filename} æ£€æµ‹ç»“æœ:")
            for issue_type, count in detected_issues_by_type.items():
                logger.info(f"    - {issue_type}: {count}ä¸ª")
            logger.info(f"  âœ… æ€»è®¡å‘ç° {len(issues)} ä¸ªåŸºç¡€é—®é¢˜")
        else:
            logger.info(f"  âœ… {file_patch.filename} æœªå‘ç°åŸºç¡€é—®é¢˜")
        
        return issues
    
    def _detect_security_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹å®‰å…¨é—®é¢˜"""
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        security_patterns = {
            "sql_injection": {
                "patterns": [r"execute\(.*\+.*\)", r"query\(.*\+.*\)", r"SELECT.*\+"],
                "description": "å¯èƒ½å­˜åœ¨SQLæ³¨å…¥é£é™©ï¼Œæ£€æµ‹åˆ°å­—ç¬¦ä¸²æ‹¼æ¥æ„å»ºSQLæŸ¥è¯¢",
                "suggestion": "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æˆ–é¢„ç¼–è¯‘è¯­å¥é˜²æ­¢SQLæ³¨å…¥"
            },
            "xss": {
                "patterns": [r"innerHTML\s*=", r"document\.write\(", r"eval\("],
                "description": "å¯èƒ½å­˜åœ¨è·¨ç«™è„šæœ¬(XSS)é£é™©ï¼Œæ£€æµ‹åˆ°ç›´æ¥æ“ä½œDOMæˆ–æ‰§è¡ŒåŠ¨æ€ä»£ç ",
                "suggestion": "å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œè½¬ä¹‰ï¼Œä½¿ç”¨å®‰å…¨çš„DOMæ“ä½œæ–¹æ³•"
            },
            "hardcoded_secrets": {
                "patterns": [r"password\s*=\s*['\"]", r"api[_-]?key\s*=\s*['\"]", r"secret\s*=\s*['\"]"],
                "description": "æ£€æµ‹åˆ°ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯ï¼Œå­˜åœ¨å®‰å…¨æ³„éœ²é£é™©",
                "suggestion": "å°†æ•æ„Ÿä¿¡æ¯å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡æˆ–å®‰å…¨çš„é…ç½®ç®¡ç†ç³»ç»Ÿä¸­"
            },
            "path_traversal": {
                "patterns": [r"\.\.\/", r"\.\.\\\\", r"os\.path\.join.*\.\."],
                "description": "å¯èƒ½å­˜åœ¨è·¯å¾„ç©¿è¶Šé£é™©ï¼Œæ£€æµ‹åˆ°ç›¸å¯¹è·¯å¾„æ“ä½œ",
                "suggestion": "éªŒè¯å’Œæ¸…ç†æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–å®‰å…¨çš„è·¯å¾„å¤„ç†æ–¹æ³•"
            },
            "command_injection": {
                "patterns": [r"os\.system\(", r"subprocess\.", r"exec\(", r"shell=True"],
                "description": "å¯èƒ½å­˜åœ¨å‘½ä»¤æ³¨å…¥é£é™©ï¼Œæ£€æµ‹åˆ°ç³»ç»Ÿå‘½ä»¤æ‰§è¡Œ",
                "suggestion": "é¿å…æ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‘½ä»¤ï¼Œä½¿ç”¨å‚æ•°åŒ–çš„å‘½ä»¤æ‰§è¡Œæˆ–ç™½åå•éªŒè¯"
            }
        }
        
        for line_num, line in enumerate(new_lines, 1):
            for category, config in security_patterns.items():
                if any(re.search(pattern, line, re.IGNORECASE) for pattern in config["patterns"]):
                    issues.append({
                        "type": "potential_security_issue",
                        "filename": file_patch.filename,
                        "line_number": line_num,
                        "description": f"{config['description']}ã€‚ä»£ç ï¼š{line.strip()[:50]}{'...' if len(line.strip()) > 50 else ''}",
                        "suggestion": config["suggestion"],
                        "severity": "high",
                        "category": category
                    })
        
        return issues
    
    def _detect_performance_issues(self, file_patch: FilePatchInfo) -> List[Dict]:
        """æ£€æµ‹æ€§èƒ½é—®é¢˜"""
        issues = []
        new_lines = [line for line in file_patch.patch.split('\n') if line.startswith('+')]
        
        performance_patterns = {
            "n_plus_1_query": {
                "patterns": [r"for.*in.*:", r"\.get\(", r"\.filter\("],
                "description": "å¯èƒ½å­˜åœ¨N+1æŸ¥è¯¢é—®é¢˜ï¼Œåœ¨å¾ªç¯ä¸­è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢"
            },
            "inefficient_loop": {
                "patterns": [r"for.*in.*for.*in", r"while.*while"],
                "description": "æ£€æµ‹åˆ°åµŒå¥—å¾ªç¯ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜"
            },
            "memory_leak": {
                "patterns": [r"\.append\(", r"global\s+", r"cache\["],
                "description": "å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼é£é™©ï¼Œæ£€æµ‹åˆ°æŒç»­å¢é•¿çš„æ•°æ®ç»“æ„"
            },
            "blocking_io": {
                "patterns": [r"requests\.get", r"urllib\.request", r"time\.sleep"],
                "description": "æ£€æµ‹åˆ°é˜»å¡I/Oæ“ä½œï¼Œå¯èƒ½å½±å“æ€§èƒ½"
            },
            "inefficient_data_structure": {
                "patterns": [r"list\(\)", r"dict\(\)", r"\[\].*in.*for"],
                "description": "å¯èƒ½ä½¿ç”¨äº†ä½æ•ˆçš„æ•°æ®ç»“æ„æˆ–æ“ä½œ"
            }
        }
        
        for line_num, line in enumerate(new_lines, 1):
            for issue_type, config in performance_patterns.items():
                if any(re.search(pattern, line, re.IGNORECASE) for pattern in config["patterns"]):
                    issues.append({
                        "type": issue_type,
                        "filename": file_patch.filename,
                        "line_number": line_num,
                        "description": f"{config['description']}ã€‚ä»£ç ï¼š{line.strip()[:50]}{'...' if len(line.strip()) > 50 else ''}",
                        "severity": "medium",
                        "suggestion": self._get_performance_suggestion(issue_type)
                    })
        
        return issues
    
    async def _ai_comprehensive_analysis(self, diff_files: List[FilePatchInfo], mr_info: Dict) -> Dict[str, Any]:
        """AIç»¼åˆåˆ†æ"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI comprehensive analysis")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡AIåˆ†æ"}
        
        # æ„å»ºåˆ†ææç¤ºè¯
        file_summaries = []
        for file_patch in diff_files[:10]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
            summary = f"æ–‡ä»¶: {file_patch.filename}\n"
            summary += f"å˜æ›´ç±»å‹: {file_patch.edit_type}\n"
            summary += f"å˜æ›´å†…å®¹:\n{file_patch.patch[:1000]}...\n"  # é™åˆ¶é•¿åº¦
            file_summaries.append(summary)
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
è¯·åˆ†æä»¥ä¸‹GitLab Merge Requestçš„ä»£ç å˜æ›´ï¼š

MRä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- æºåˆ†æ”¯ï¼š{mr_info.get('source_branch', 'æœªçŸ¥')}
- ç›®æ ‡åˆ†æ”¯ï¼š{mr_info.get('target_branch', 'æœªçŸ¥')}

æ–‡ä»¶å˜æ›´ï¼š
{chr(10).join(file_summaries)}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µ
2. æ½œåœ¨çš„bugå’Œé—®é¢˜
3. æ€§èƒ½å½±å“
4. å®‰å…¨è€ƒè™‘
5. å¯ç»´æŠ¤æ€§
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š
{
    "findings": [
        {
            "type": "é—®é¢˜ç±»å‹",
            "filename": "æ–‡ä»¶å", 
            "line_number": è¡Œå·,
            "severity": "high/medium/low",
            "description": "é—®é¢˜æè¿°",
            "suggestion": "ä¿®å¤å»ºè®®"
        }
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
    "overall_assessment": "æ•´ä½“è¯„ä¼°"
}
"""
        else:
            prompt = base_prompt + "\nè¯·æä¾›è¯¦ç»†çš„ä»£ç å®¡æŸ¥åˆ†æã€‚"
        
        try:
            response = await self.client.chat_completion(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·ä»”ç»†åˆ†æä»£ç å¹¶æä¾›å»ºè®¾æ€§çš„åé¦ˆã€‚"},
                    {"role": "user", "content": prompt}
                ],
                model=self.model,
                temperature=0.2,
                max_tokens=4000,
                response_format=CODE_REVIEW_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            logger.warning(f"AI response is not valid JSON: {e}")
            logger.debug(f"Raw response: {response[:500]}...")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIåˆ†æå¤±è´¥"}
        except Exception as e:
            logger.error(f"Error processing AI response: {e}")
            return {"findings": [], "suggestions": [], "overall_assessment": "AIåˆ†æå¤„ç†å¤±è´¥"}
    
    def _extract_json_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹ï¼Œå¢å¼ºå®¹é”™æ€§"""
        import re
        
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        response = response.strip()
        logger.debug(f"Processing response: {response[:200]}...")
        
        # å¦‚æœå“åº”åŒ…å«```jsonæ ‡è®°ï¼Œæå–å…¶ä¸­çš„JSON
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            if end != -1:
                json_content = response[start:end].strip()
                logger.debug("Extracted JSON from ```json markdown block")
                return self._fix_common_json_issues(json_content)
        
        # å¦‚æœå“åº”åŒ…å«```æ ‡è®°ä½†æ²¡æœ‰jsonæ ‡è¯†ï¼Œä¹Ÿå°è¯•æå–
        if response.startswith("```") and response.endswith("```"):
            lines = response.split('\n')
            if len(lines) > 2:
                json_content = '\n'.join(lines[1:-1]).strip()
                logger.debug("Extracted content from generic code block")
                return self._fix_common_json_issues(json_content)
        
        # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
        start_idx = response.find('{')
        if start_idx != -1:
            # ä»ç¬¬ä¸€ä¸ª{å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„}
            brace_count = 0
            for i, char in enumerate(response[start_idx:], start_idx):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_content = response[start_idx:i+1]
                        logger.debug("Extracted JSON object from response")
                        return self._fix_common_json_issues(json_content)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONç»“æ„ï¼Œå°è¯•ä¿®å¤å¹¶è¿”å›
        logger.debug("No clear JSON structure found, attempting to fix response")
        return self._fix_common_json_issues(response)
    
    def _fix_common_json_issues(self, json_str: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜"""
        import re
        
        # ç§»é™¤å‰åç©ºç™½
        json_str = json_str.strip()
        
        # å¦‚æœä¸æ˜¯ä»¥{å¼€å¤´ï¼Œå°è¯•æ‰¾åˆ°å¹¶æˆªå–JSONéƒ¨åˆ†
        if not json_str.startswith('{'):
            match = re.search(r'\{.*\}', json_str, re.DOTALL)
            if match:
                json_str = match.group(0)
        
        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯
        # 1. ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·
        json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)  # 'key': -> "key":
        json_str = re.sub(r":\s*'([^']*)'", r': "\1"', json_str)  # : 'value' -> : "value"
        
        # 2. ä¿®å¤æœªå¼•ç”¨çš„é”®å
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)
        
        # 3. ä¿®å¤å°¾éšé€—å·
        json_str = re.sub(r',\s*}', '}', json_str)
        json_str = re.sub(r',\s*]', ']', json_str)
        
        # 4. ç¡®ä¿å­—ç¬¦ä¸²å€¼è¢«æ­£ç¡®å¼•ç”¨
        # è¿™ä¸ªæ¯”è¾ƒå¤æ‚ï¼Œå…ˆè·³è¿‡é«˜çº§ä¿®å¤
        
        logger.debug(f"Fixed JSON: {json_str[:200]}...")
        return json_str
    
    def _aggressive_json_fix(self, json_str: str) -> str:
        """æ¿€è¿›çš„JSONä¿®å¤æ–¹æ³•"""
        import re
        
        logger.debug("å°è¯•æ¿€è¿›JSONä¿®å¤...")
        
        # å¦‚æœå®Œå…¨è§£æå¤±è´¥ï¼Œå°è¯•æ„å»ºä¸€ä¸ªæœ€å°çš„æœ‰æ•ˆJSON
        if not json_str or json_str.strip() == "":
            return '{"findings": [], "suggestions": []}'
        
        # å°è¯•æå–findingså’Œsuggestionsçš„å†…å®¹
        findings_match = re.search(r'"findings"\s*:\s*\[(.*?)\]', json_str, re.DOTALL)
        suggestions_match = re.search(r'"suggestions"\s*:\s*\[(.*?)\]', json_str, re.DOTALL)
        
        findings_content = findings_match.group(1) if findings_match else ""
        suggestions_content = suggestions_match.group(1) if suggestions_match else ""
        
        # æ„å»ºä¸€ä¸ªæ–°çš„JSON
        fixed_json = f'{{"findings": [{findings_content}], "suggestions": [{suggestions_content}]}}'
        
        # å†æ¬¡åº”ç”¨åŸºç¡€ä¿®å¤
        return self._fix_common_json_issues(fixed_json)
    
    def _validate_and_fix_result(self, result: dict) -> dict:
        """éªŒè¯å¹¶ä¿®å¤ç»“æœç»“æ„"""
        # ç¡®ä¿å¿…éœ€çš„å­—æ®µå­˜åœ¨
        if not isinstance(result, dict):
            logger.warning("ç»“æœä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè¿”å›é»˜è®¤ç»“æ„")
            return {"findings": [], "suggestions": []}
        
        # ç¡®ä¿findingsæ˜¯åˆ—è¡¨
        if "findings" not in result or not isinstance(result["findings"], list):
            logger.warning("findingså­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
            result["findings"] = []
        
        # ç¡®ä¿suggestionsæ˜¯åˆ—è¡¨
        if "suggestions" not in result or not isinstance(result["suggestions"], list):
            logger.warning("suggestionså­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
            result["suggestions"] = []
        
        # éªŒè¯æ¯ä¸ªfindingçš„æ ¼å¼
        valid_findings = []
        for i, finding in enumerate(result["findings"]):
            if not isinstance(finding, dict):
                logger.warning(f"Finding {i} ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                continue
            
            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            if "type" not in finding:
                finding["type"] = "unknown"
            if "severity" not in finding:
                finding["severity"] = "low"
            if "description" not in finding:
                finding["description"] = "æœªæä¾›æè¿°"
            
            # ç¡®ä¿severityå€¼æœ‰æ•ˆ
            if finding["severity"] not in ["high", "medium", "low"]:
                finding["severity"] = "low"
            
            valid_findings.append(finding)
        
        result["findings"] = valid_findings
        
        # éªŒè¯suggestions
        valid_suggestions = []
        for suggestion in result["suggestions"]:
            if isinstance(suggestion, str) and suggestion.strip():
                valid_suggestions.append(suggestion.strip())
        result["suggestions"] = valid_suggestions
        
        logger.debug(f"éªŒè¯åçš„ç»“æœ: {len(result['findings'])} findings, {len(result['suggestions'])} suggestions")
        return result
    
    async def _ai_security_analysis(self, filename: str, line_num: int, code_line: str, category: str) -> Dict[str, Any]:
        """AIå®‰å…¨åˆ†æ"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI security analysis")
            return {
                "is_vulnerability": False,
                "risk_level": 0,
                "description": "AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒAIå®‰å…¨åˆ†æ",
                "filename": filename,
                "line_number": line_num,
                "category": category
            }
            
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
åˆ†æä»¥ä¸‹ä»£ç è¡Œæ˜¯å¦å­˜åœ¨å®‰å…¨æ¼æ´ï¼š

æ–‡ä»¶ï¼š{filename}
è¡Œå·ï¼š{line_num}  
ä»£ç ï¼š{code_line}
å¯èƒ½é—®é¢˜ï¼š{category}

è¯·åˆ†æï¼š
1. è¿™æ˜¯å¦çœŸçš„æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Ÿ
2. é£é™©çº§åˆ«ï¼ˆ1-10ï¼‰
3. å…·ä½“çš„å®‰å…¨é£é™©
4. ä¿®å¤å»ºè®®
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
{
    "is_vulnerability": true/false,
    "risk_level": 1-10,
    "description": "è¯¦ç»†æè¿°",
    "fix_suggestion": "ä¿®å¤å»ºè®®"
}
"""
        else:
            prompt = base_prompt
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.1,
                max_tokens=500,
                response_format=SECURITY_ANALYSIS_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            result = json.loads(cleaned_response)
            result.update({
                "filename": filename,
                "line_number": line_num,
                "category": category
            })
            return result
            
        except Exception as e:
            logger.error(f"Failed to analyze security issue: {e}")
            return {
                "is_vulnerability": False,
                "risk_level": 0,
                "description": "åˆ†æå¤±è´¥",
                "filename": filename,
                "line_number": line_num,
                "category": category
            }
    
    async def _ai_performance_analysis(self, issues: List[Dict], diff_files: List[FilePatchInfo]) -> List[Dict]:
        """AIæ€§èƒ½åˆ†æ"""
        if not issues:
            return []
            
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, skipping AI performance analysis")
            return []
        
        # æ„å»ºæ€§èƒ½åˆ†ææç¤º
        issues_summary = "\n".join([
            f"- {issue['type']} in {issue['filename']}:{issue['line_number']}"
            for issue in issues[:5]
        ])
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""
æ£€æµ‹åˆ°ä»¥ä¸‹æ€§èƒ½é—®é¢˜ï¼š
{issues_summary}

è¯·åˆ†æè¿™äº›é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
"""
        
        # å¦‚æœä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œæ·»åŠ JSONæ ¼å¼è¯´æ˜
        if not self.client._supports_structured_output(self.model):
            prompt = base_prompt + """
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ•°ç»„æ ¼å¼å›å¤ï¼Œæ¯ä¸ªé—®é¢˜ä¸€ä¸ªå¯¹è±¡ï¼š
[
    {
        "type": "æ€§èƒ½é—®é¢˜ç±»å‹",
        "severity": "high/medium/low", 
        "description": "é—®é¢˜æè¿°",
        "optimization": "ä¼˜åŒ–å»ºè®®"
    }
]
"""
        else:
            prompt = base_prompt
        
        try:
            response = await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.2,
                max_tokens=1000,
                response_format=PERFORMANCE_ANALYSIS_SCHEMA
            )
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå–JSONéƒ¨åˆ†
            cleaned_response = self._extract_json_from_response(response)
            return json.loads(cleaned_response)
        except Exception as e:
            logger.error(f"Performance analysis failed: {e}")
            return []
    
    async def _ai_quick_summary(self, diff_files: List[FilePatchInfo], mr_info: Dict) -> str:
        """AIå¿«é€Ÿæ€»ç»“"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self._is_ai_available():
            logger.info("AI client not available, using basic summary")
            files_count = len(diff_files)
            return f"å¿«é€Ÿåˆ†æå®Œæˆï¼Œå˜æ›´æ¶‰åŠ{files_count}ä¸ªæ–‡ä»¶ã€‚AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä»…æä¾›åŸºç¡€åˆ†æã€‚"
            
        files_info = ", ".join([f.filename for f in diff_files[:5]])
        if len(diff_files) > 5:
            files_info += f" ç­‰{len(diff_files)}ä¸ªæ–‡ä»¶"
        
        prompt = f"""
å¿«é€Ÿæ€»ç»“ä»¥ä¸‹MRçš„å˜æ›´ï¼š
- æ ‡é¢˜ï¼š{mr_info.get('title', 'æœªçŸ¥')}
- å˜æ›´æ–‡ä»¶ï¼š{files_info}
- æ€»æ–‡ä»¶æ•°ï¼š{len(diff_files)}

è¯·ç”¨1-2å¥è¯æ€»ç»“ä¸»è¦å˜æ›´å†…å®¹å’Œå½±å“ã€‚
"""
        
        try:
            return await self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.3,
                max_tokens=200
            )
        except Exception as e:
            logger.error(f"Quick summary failed: {e}")
            return f"å¿«é€Ÿåˆ†æå®Œæˆï¼Œå˜æ›´æ¶‰åŠ{len(diff_files)}ä¸ªæ–‡ä»¶ã€‚"
    
    def _calculate_comprehensive_score(self, findings: List[Dict], diff_files: List[FilePatchInfo]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        base_score = 8.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for finding in findings:
            severity = finding.get("severity", "low")
            if severity == "high":
                base_score -= 1.0
            elif severity == "medium":
                base_score -= 0.5
            else:
                base_score -= 0.2
        
        # æ ¹æ®æ–‡ä»¶æ•°é‡è°ƒæ•´
        if len(diff_files) > 10:
            base_score -= 0.5
        
        return max(base_score, 3.0)  # æœ€ä½3åˆ†
    
    def _calculate_security_score(self, security_issues: List[Dict]) -> float:
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        base_score = 9.0
        
        for issue in security_issues:
            risk_level = issue.get("risk_level", 0)
            base_score -= risk_level * 0.3
        
        return max(base_score, 1.0)
    
    def _calculate_performance_score(self, performance_issues: List[Dict]) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        base_score = 8.0
        
        for issue in performance_issues:
            severity = issue.get("severity", "low")
            if severity == "high":
                base_score -= 1.5
            elif severity == "medium":
                base_score -= 0.8
            else:
                base_score -= 0.3
        
        return max(base_score, 2.0)
    
    def _generate_summary(self, findings: List[Dict], score: float, files_count: int) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        high_issues = len([f for f in findings if f.get("severity") == "high"])
        medium_issues = len([f for f in findings if f.get("severity") == "medium"])
        
        if score >= 8.0:
            quality = "ä¼˜ç§€"
        elif score >= 6.0:
            quality = "è‰¯å¥½"
        elif score >= 4.0:
            quality = "ä¸€èˆ¬"
        else:
            quality = "éœ€è¦æ”¹è¿›"
        
        return f"ä»£ç è´¨é‡è¯„ä¼°ï¼š{quality}ã€‚åˆ†æäº†{files_count}ä¸ªæ–‡ä»¶ï¼Œå‘ç°{high_issues}ä¸ªé«˜é£é™©é—®é¢˜ï¼Œ{medium_issues}ä¸ªä¸­ç­‰é£é™©é—®é¢˜ã€‚"
    
    def _generate_recommendations(self, findings: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if any(f.get("type") == "debug_statement" for f in findings):
            recommendations.append("ç§»é™¤è°ƒè¯•è¯­å¥å’Œæ—¥å¿—è¾“å‡º")
        
        if any(f.get("type") == "line_too_long" for f in findings):
            recommendations.append("ä¿æŒä»£ç è¡Œé•¿åº¦åœ¨åˆç†èŒƒå›´å†…")
        
        if any(f.get("severity") == "high" for f in findings):
            recommendations.append("ä¼˜å…ˆå¤„ç†é«˜é£é™©é—®é¢˜")
        
        recommendations.append("å»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–å˜æ›´ä»£ç ")
        recommendations.append("ç¡®ä¿ä»£ç éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒ")
        
        return recommendations
    
    def _generate_security_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        recommendations = []
        
        categories = set(issue.get("category") for issue in issues)
        
        if "sql_injection" in categories:
            recommendations.append("ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥")
        if "xss" in categories:
            recommendations.append("å¯¹è¾“å‡ºå†…å®¹è¿›è¡Œé€‚å½“è½¬ä¹‰")
        if "hardcoded_secrets" in categories:
            recommendations.append("å°†æ•æ„Ÿä¿¡æ¯å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­")
        if "command_injection" in categories:
            recommendations.append("é¿å…ç›´æ¥æ‰§è¡Œç”¨æˆ·è¾“å…¥çš„å‘½ä»¤")
        
        return recommendations
    
    def _generate_performance_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []
        
        types = set(issue.get("type") for issue in issues)
        
        if "n_plus_1_query" in types:
            recommendations.append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œé¿å…N+1é—®é¢˜")
        if "inefficient_loop" in types:
            recommendations.append("ä¼˜åŒ–å¾ªç¯é€»è¾‘ï¼Œå‡å°‘åµŒå¥—å±‚çº§")
        if "blocking_io" in types:
            recommendations.append("è€ƒè™‘ä½¿ç”¨å¼‚æ­¥I/Oæ“ä½œ")
        if "memory_leak" in types:
            recommendations.append("æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼")
        
        return recommendations
    
    def _get_performance_suggestion(self, issue_type: str) -> str:
        """è·å–æ€§èƒ½é—®é¢˜å»ºè®®"""
        suggestions = {
            "n_plus_1_query": "è€ƒè™‘ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢æˆ–é¢„åŠ è½½",
            "inefficient_loop": "å°è¯•ä¼˜åŒ–ç®—æ³•æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„",
            "memory_leak": "ç¡®ä¿åŠæ—¶æ¸…ç†ä¸éœ€è¦çš„å¯¹è±¡å¼•ç”¨",
            "blocking_io": "ä½¿ç”¨å¼‚æ­¥æ“ä½œæˆ–çº¿ç¨‹æ± ",
            "inefficient_data_structure": "é€‰æ‹©æ›´é€‚åˆçš„æ•°æ®ç»“æ„"
        }
        return suggestions.get(issue_type, "è€ƒè™‘ä¼˜åŒ–æ­¤å¤„çš„æ€§èƒ½")
    
    def _estimate_analysis_cost(self, diff_files: List[FilePatchInfo]) -> float:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        total_tokens = 0
        for file_patch in diff_files:
            total_tokens += self.token_manager.count_tokens(file_patch.patch)
        
        return self.token_manager.estimate_cost(total_tokens)